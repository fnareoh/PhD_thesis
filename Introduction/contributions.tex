 \section{Contributions}\label{intro:sec:contrib}

 \todo[inline]{If I have time to write up the streaming gapped, add a description of it, else remove it of the rest of the introduction.}

So far, we presented the two core challenges at the heart of text processing: enabling relevant (and sometimes complex) queries suited to specific applications while also maintaining performances that can scale to large volumes of data.
%
This thesis makes theoretical and practical contributions to address both needs. 
Each chapter is  an independent publication, with a specific problem, a review of the state-of-the-art, and contribution. This choice was motivated by the variety of subjects and techniques used, and how they were conceived as independent projects (with varying sets of co-authors) during the Ph.D. However, this section intends to provide an overview of the main contributions of the thesis, some insights into the techniques used, and how they relate to the concept of sketches.

Part~\ref{part:complex_queries} focuses on a theoretical study of complex queries. 
%
% Regular expressions
We start with regular expression search in the streaming setting.
%
As mentioned previously, the streaming model considers that the regular expression $R$ we are searching for and the length of the stream $n$ are given in advance and can be preprocessed, then the text $T$ starts arriving character by character. For regular expression \emph{membership} we need to determine, after having seen $T$ entirely, whether it matches a regular expression $R$, while for \emph{pattern matching}, we must answer, at each position $r$, whether there exists a substring $T[l..r]$ recognized by $R$.
% Talk about the lowerbounds ?
% Main contribution
In \textbf{Chapter~\ref{chap:regexp}}, our main contribution is to identify $d$, the number of union symbols and Kleene stars in $R$, as the key parameter that enables a space-efficient streaming algorithm. We design randomized Monte Carlo algorithms (meaning the execution time is deterministic, but the algorithms can err with a small probability) that solve regular expression membership and pattern matching in $\Oh(d^3\polylog n)$ space and $\Oh(nd^5\polylog n)$ time per character of $T$ (Theorem~\ref{th:memb}).
% Previous uses of the parameter
Bille and Thorup~\cite{doi:10.1137/1.9781611973075.104}\footnote{They actually consider $k$ the number of strings appearing in $R$ but $k=\Theta(d)$.} already used this parameter to  show algorithms (not in the streaming model) to solve membership and pattern matching in $\Oh(m)$ space and $\Oh(n(\frac{d\log w}{w} + \log d))$ time, where $w$ is the size of the machine word.
% Links to previous results in streaming
In the streaming model, this parameter was already known for the two special cases of streaming dictionary matching and don't care matching. To match a dictionary of patterns $\{P_1, P_2, ... P_d \}$ of length at most $m$, which corresponds to pattern matching for the regular expression $(P_1| P_2| ... | P_d)$, a series of results~\cite{Porat:09,DBLP:journals/talg/BreslauerG14,DBLP:conf/esa/CliffordFPSS15,DBLP:conf/esa/GolanP17,DBLP:conf/icalp/GolanKP18} led to the development of a randomized Monte Carlo algorithm in $\Oh(d\log m)$ space and $\Oh(\log \log |\Sigma|)$ time per character.
Don't care pattern matching for a pattern $P_1 ? P_2 ... ? P_d$ where $P_i$, $i \in [1,d]$, are strings (possibly empty) over $\Sigma$ of total length at most $m$, can be expressed as matching $R = P_1 (1|2|\ldots|\sigma) P_2 (1|2|\ldots|\sigma) \ldots (1|2|\ldots|\sigma) P_{d}$, and Golan, Kopelowitz, and Porat~\cite{DBLP:journals/algorithmica/GolanKP19} showed that this problem can be solved by a randomized Monte Carlo algorithm in $\Oh(d \log m)$ space and $\Oh(d+\log m)$ time per character.

% Intuitions of the ideas and how it links to sketches.
Here is a bird's eye view of how we prove our result: we start by defining \emph{atomic strings} which are the strings obtained by just keeping the concatenation and splitting at union, Kleene star, and parentheses. They only contain characters of $\Sigma$ and there are $\Theta(d)$ of them. For example, for $R= \mathrm{GAT}(\mathrm{TA}\mid \mathrm{O})(\mathrm{CAT})^*$  the set of atomic strings is $\{$GAT, TA, O, CAT$\}$.
%
The basis of our approach is to efficiently store some specific occurrences of prefixes of the atomic strings in the text $T$. Those stored occurrences are then linked to test if there is a “partial” match of $R$ (Definition~\ref*{def:partial_occ_regexp}). We store only a few occurrences over periodic regions of the text, thus the stored occurrences can be very far apart, with just a long periodic substring in between. To reconstruct a partial match, we must check if that long periodic substring corresponds to a run of the Thompson automaton. We formulate it as finding a path of a specific weight in a multi-graph. We then efficiently solve this graph problem by translating it into a circuit using addition and convolution gates that can be evaluated in a space-efficient manner using a general framework~\cite{LokshtanovN10,Bringmann17}. Additionally, we improve that framework by removing its dependency on the Extended Riemann Hypothesis (Theorem~\ref{thm:bombieri}). 
%Sketches
Here the key innovation relating to sketches is how the specific occurrences we store are chosen. Due to properties of periodicity (see Preliminaries~\ref{sec:prelim:FW}), it is common in streaming algorithms to handle the aperiodic and periodic strings as two separate cases.
Here we choose to apply this reasoning recursively on $\Oh(\log n)$ levels through the notion of the ``anchor'' position (Definition~\ref{def:anchors}). For each atomic string (or a prefix of it) and anchor position, either there are just a few occurrences crossing the anchor, and we can afford to store them, or the region is periodic, and we can look at an earlier position instead. This is key for achieving the desired space complexity and to storing the necessary occurrences.


% Gapped
In \textbf{Chapter~\ref{chap:gapped_index}}, we begin our study of gapped consecutive matching. Recall that we are given $P_1$, $P_2$, and an interval $[a,b]$ and must report all pairs of positions $(i,j)$ in~$T$ such that an occurrence of $P_1$ starts at position $i$, an occurrence of $P_2$ starts at position $j$, there are no occurrences of $P_1$ or $P_2$ starting in the interval $[i+1,j-1]$, and finally $j-i \in [a,b]$.
% Give specific motivations of the model ?
% Indexing
% Motivation to study indexing 
Bille et al.~\cite{bille2022gapped} introduced those queries and gave a conditional lower bound stating that for indexes of size $\tilde \Oh(|T|)$ (the $\tilde \Oh$ notation hides logarithmic factors) achieving a query time faster than $\tilde \Oh(|P_1|+|P_2|+\sqrt{|T|})$ would contradict the Set Disjointness conjecture, even if $a=0$ is fixed. Additionally, they provide a non-trivial upper bound that uses $\tilde \Oh(|T|)$ space and $\tilde \Oh (|P_1| +|P_2| + |T|^{2/3}\occ^{1/3})$ time to report all $\occ$ occurrences. 
% Our contributions
We study the case where $a=0$ is fixed, and the text is given as a straight-line program (SLP), and our contribution is to create an index taking space polynomial in the size of the grammar that answers such queries in optimal time up to poly log factors.
To report consecutive occurrences without constraints on the distance between $P_1$ and $P_2$, our index uses $\Oh(g^2\log^4|T|)$ space where $g$ is the size of the SLP (see Corollary~\ref{cor:all}).
% Insights
This is obtained through Theorem~\ref{th:occurrences} where we provide an efficient data structure to compute several types of queries. We rely on an efficient implementation of compact tries (see Preliminaries~\ref{sec:prelim:tries}) which takes advantage of the strings being prefixes and suffixes of the non-terminals. This implementation uses Karp--Rabin Fingerprints (see Preliminaries~\ref{sec:prelim:KR}) and the compacted tries are then augmented using a heavy path decomposition (see Preliminaries~\ref{sec:prelim:HP}). This construction forms a sketch tailored to the grammar compression and the specific queries needed for consecutive occurrences.
Theorem~\ref{th:occurrences} is then reused for our main result: Theorem~\ref{thm:close_co_occurrences}, with an index that can report consecutive occurrences with distances in an interval $[0,b]$ using $\Oh (g^5\log^5(|T|))$ space.
% Comments
This index circumvents the lower bound for highly compressible texts, which was not a given considering that some problems cannot avoid a high dependency on the size of the uncompressed string (as detailed in Section~\ref{sec:intro:scalability}). However, we expect our space complexity to be far from optimal and leave improvements as well as the general case with $0 \leq a \leq b \leq |T|$\footnote{We started writing a solution for the general case, but it was very technical and had an unreasonable space-complexity of $\tilde \Oh(g^{15})$...} as open questions.

% PM
Partially motivated by the limitations of a space usage in $\tilde \Oh(g^5)$, in \textbf{Chapter~\ref{chap:gapped_pm}}, we address the dual problem: consecutive pattern matching, where the patterns and the text are processed simultaneously. Note that for an uncompressed text, consecutive pattern matching can be solved by a classic online matching algorithm, just keeping track of the most recent occurrences of $P_1$ and $P_2$ in $\Oh(|T|+|P_1|+|P_2|+\occ)$ time.
% Results
We show that a similar complexity can be achieved when the text is highly compressible: all consecutive occurrences can be reported in $\Oh(g+|P_1|+|P_2|+\occ)$ time (see Theorem~\ref{th:main}) where $g$ is the size of the grammar compressed text. We then derive from this result algorithms for gapped consecutive matching (Corollary~\ref{cor:ab}) and the k-closest consecutive occurrences (Corollary~\ref{cor:topk}).
% Insights
Recently, Ganardi and Gawrychowski~\cite{DBLP:conf/soda/GanardiG22} introduced an efficient sketch for grammar compressed pattern matching: boundary information. For a given pattern $p$, the $p$-boundary information of a string $s$ stores either one $p$-substring information if $s$ occurs in $p$, or several substrings of $p$ that contain the longest prefix of $s$ that is a suffix of $p$ ($p$-prefix information) and several substrings of $p$ that contain the longest suffix of $s$ that is a prefix of $p$. The authors show how to use this sketch to determine in $\Oh(g+|P|)$ whether $P$ occurs in the compressed text. We sightly extend their approach to report all occurrences crossing the boundary (Lemma~\ref{lemma:crossing}). Then by repeating this technique on a second level, we create a similar sketch which we call ``secondary boundary information'' and by a careful case-analysis, we obtain Theorem~\ref{th:main}.

% Squares
All previous chapters rely heavily on periodicity detection 
%the Fine-Wilf periodicity lemma and its corollaries 
(see Preliminaries~\ref{sec:prelim:FW}) to efficiently represent occurrences of a string crossing a position, thus it felt only natural to also study periodicity detection. In \textbf{Chapter~\ref{chap:squares}} we show how to report all runs  in optimal time in the most abstract model where they can be defined: General (unordered) alphabets where the only operation allowed is an equality test between two characters. 
We first focus on the problem of square detection, then extend our approach to square and run reporting.
% Kown results and open questions
In 1984, Main and Lorentz~\cite{Main1984} designed an $\Oh(n\log n)$ time algorithm for square detection in a text $T$ of size $n$ over a general unordered alphabet. They also provided a matching lower bound for strings that have $\Omega(n)$ distinct symbols but left as an open question whether a faster algorithm was possible if the size of the alphabet $\sigma=|\Sigma|$ is restricted.
% Lowerbounds
We start by proving that the problem requires $\Omega(n \log \sigma)$ comparison even if the size of the alphabet is known (Theorem~\ref{thm:lowerbound}). Additionally, in Theorem~\ref{thm:inapproxalph}, we show that computing any relevant approximation of the number of distinct characters requires $\Omega(n\sigma)$ operations.
% General ordered
For general ordered alphabets (where an order is given), Crochemore~\cite{Crochemore1986} used the $f$-factorization (related to popular Lempel--Ziv factorization) to give an algorithm for square detection running in $\Oh(n\log \sigma)$ time. Roughly speaking, the $f$-factorization and Lempel--Ziv factorization (LZ factorisation) detect repetitive fragments in the text and can be computed efficiently using a suffix tree or suffix array. However,  we show that on general unordered alphabets, those factorization require $\Omega(n\sigma)$ operations to be computed (as a corollary of the lower bound on approximating the alphabet). %, where $\sigma$ is the number of distinct characters in $T$.
% Delta approximate factorization sketch
Instead, we introduce the $\Delta$-approximate LZ factorization which acts as a sketch capturing only the sufficiently long squares (of length at least $8\Delta$), as opposed to the $f$-factorization and LZ factorization that capture all squares.
We present our final algorithm in steps. We first assume that the alphabet size is known and focus on giving an upper bound on the number of comparisons, then we proceed to remove the assumption on knowing the alphabet size, and finally we provide an overall efficient algorithm running in time $\Oh(n\log\sigma)$.


% Part two

% LCS

% DTW

% XBWT
 \section{Contributions}\label{intro:sec:contrib}

 \todo[inline]{Work In Progress, do not review}

So far, we presented the two core challenges at the heart of text processing: enabling relevant (and sometimes complex) queries suited to specific applications and while maintaining performances that can scale to the large volumes of input data.
%
This thesis makes theoretical and practical contributions to address both needs. 
Each contribution is presented as an independent chapter corresponding to a publication. This choice was motivated by the variety of subjects and techniques as well as how they were conceived as independent projects (with varying sets of co-authors) during the Ph.D. This section is meant to give an overview of the main contributions of the thesis and how they relate to the concept of sketches.

\todo[inline]{If I have time to write up the streaming gapped, add a description of it}

Part~\ref{part:complex_queries} focuses on a theoretical study of complex queries. 
%
% Regular expressions
We start with regular expression search in the streaming setting.
%
As mentioned previously, the streaming model considers that the regular expression $R$ we are searching for and the length of the stream $n$ are given in advance and can be preprocessed beforehand, then the text $T$ stats arriving character by character. For regular expression \emph{membership} we need to determine, after having seen $T$ entirely, whether it is recognized by $R$. While for \emph{pattern matching}, we must answer, at each position $r$, whether there exist a substring $T[l..r]$ recognized by $R$.
% Talk about the lowerbounds ?
% Main contribution
In Chapter~\ref{chap:regexp}, our main contribution is to identify $d$, the number of union symbol and Kleene star in $R$, as the key parameter that enables a space efficient streaming algorithm. We design randomized Monte Carlo algorithms (meaning the execution time is deterministic, but the algorithms can err with small probability) that solve regular expression membership and pattern matching in $\Oh(d^3\polylog n)$ space and $\Oh(nd^5\polylog n)$ time per character of $T$ (Theorem~\ref{th:memb}).
% Previous uses of the parameter
This parameter had already been used by Bille and Thorup~\cite{doi:10.1137/1.9781611973075.104}\footnote{They actually consider $k$ the number of strings appearing in $R$ but $k=\Theta(d)$.} who showed algorithms (not in streaming) for answering membership and pattern matching in $\Oh(m)$ space and $\Oh(n(\frac{d\log w}{w} + \log d))$ time with $w$ the size of the machine word.
% Links to previous results in streaming
In the streaming model, this parameter was already known for the two special cases of streaming dictionary matching and don't care matching. To match a dictionary of patterns $\{P_1, P_2, ... P_d \}$ of length at most $m$, which corresponds to pattern matching for the regular expression $(P_1| P_2| ... | P_d)$, a series of results~\cite{Porat:09,DBLP:journals/talg/BreslauerG14,DBLP:conf/esa/CliffordFPSS15,DBLP:conf/esa/GolanP17,DBLP:conf/icalp/GolanKP18} lead to a randomized Monte Carlo algorithm in $\Oh(d\log m)$ space and $\Oh(\log \log |\Sigma|)$ time per character.
While don't care matching for a pattern $P_1 ? P_2 ... ? P_d$ where $P_i$, $i \in [1,d]$, are strings (possibly empty) over $\Sigma$ of total length at most $m$, can be written as matching $R = P_1 (1|2|\ldots|\sigma) P_2 (1|2|\ldots|\sigma) \ldots (1|2|\ldots|\sigma) P_{d}$, and Golan, Kopelowitz, and Porat~\cite{DBLP:journals/algorithmica/GolanKP19} showed that this problem can be solved by a randomized Monte Carlo algorithm in $\Oh(d \log m)$ space and $\Oh(d+\log m)$ time per character.

% Intuitions of the ideas and how it links to sketches.
Here is a bird's eye view of how we prove our result: we start by defining \emph{atomic strings} which are the strings obtained by just keeping the concatenation and splitting at union, Kleene star and parentheses. They only contain characters of $\Sigma$ and there are $\Theta(d)$ of them. For example, for $R= \mathrm{GAT}(\mathrm{TA}\mid \mathrm{O})(\mathrm{CAT})^*$  the set of atomic strings is $\{$GAT,TA,O,CAT$\}$.
%
The basis of our approach is to store efficiently some specific occurrences of prefixes of the atomic strings. Those stored occurrences are then linked to test if there is a “partial” match of $R$ (Definition~\ref*{def:partial_occ_regexp}). Because we store only a few occurrences over periodic regions of the text, they can be very far apart, with just a long periodic substring in between. To reconstruct a partial match, we must check if that long periodic substring corresponds to a run of the Thompson's automaton. We formulate it as finding a path of a specific weight in a multi-graph. We then efficiently solve this graph problem by translating it into a circuit using addition and convolution gates that can be evaluated in a space-efficient manner using a general framework~\cite{LokshtanovN10,Bringmann17}. Additionally, we improve that framework by removing its dependency on the Extended Riemann Hypothesis (Theorem~\ref{thm:bombieri}). 
%Sketches
Here the key innovation relating to sketches is how the specific occurrences we store are chosen. It is common in streaming algorithms to handle the aperiodic and periodic strings as two separate cases.\todo{Explain why the aperiodic case is usually simpler}
Here we choose to apply this reasoning recursively on $\Oh(\log n)$ levels through the notion of ``anchor'' position (Definition~\ref{def:anchors}). For each atomic string (or a prefix of it) and anchor, either there are just a few occurrences crossing the anchor, and we can afford to store them, or the region is periodic, and we can look at an earlier position instead. This is key for achieving the desired space complexity and to store only the necessary occurrences.


% Gapped
In Chapter~\ref{chap:gapped_index}, we begin our study of gapped consecutive matching. Recall that we are given $P_1$, $P_2$, and interval $[a,b]$ and must report all pairs of positions $(i,j)$ in $T$ such that an occurrence of $P_1$ starts at position $i$, an occurrence $P_2$ starts at position $j$, there are no occurrences of $P_1$ or $P_2$ starting in the interval $[i+1,j-1]$, and finally $j-i \in [a,b]$.
% Give specific motivations of the model ?
% Indexing
% Motivation to study indexing 
Bille et al.~\cite{bille2022gapped} introduced those queries and gave a conditional lower bound stating that for indexes of size $\tilde \Oh(|T|)$ achieving a query time faster than $\tilde \Oh(|P_1|+|P_2|+\sqrt{|T|})$ would contradict the Set Disjointness conjecture, even if $a=0$ is fixed. Additionally, they provide a non-trivial upper bound that uses $\tilde \Oh(|T|)$ space and $\tilde \Oh (|P_1| +|P_2| + |T|^{2/3}\occ^{1/3})$ time to report all $\occ$ occurrences. 
% Our contributions
We study the case where $a=0$ is fixed, and the text is given as a straight-line program (SLP), and our contribution is to create an index taking space polynomial in the size of the grammar that answer such queries in optimal time up to poly log factors.
To report consecutive occurrences without constraints on the distance between $P_1$ and $P_2$, our index uses $\Oh(g^2\log^4|T|)$ space where $g$ is the size of the SLP (see Corollary~\ref{cor:all}).
% Insights
This is obtained through Theorem~\ref{th:occurrences} where we provide an efficient data structure to compute several efficient queries. We rely on an efficient implementation of compact tries (see preliminaries~\ref{sec:prelim:tries}) which takes advantages of the strings being prefixes and suffixes of the non-terminals. This implementation uses Karp--Rabin Fingerprints (see preliminaries~\ref{sec:prelim:KR}) and the compacted tries are then augmented using a heavy path decomposition (see preliminaries~\ref{sec:prelim:HP}). This construction forms a sketch tailored to the grammar compression and the specific queries needed for consecutive occurrences.
Theorem~\ref{th:occurrences} is then reused for our main result: Theorem~\ref{thm:close_co_occurrences}, with an index that can report consecutive occurrences with distances in an interval $[0,b]$ using $\Oh (g^5\log^5(|T|))$ space.
% Comments
Those indexes circumvent the lower bound for highly compressible texts, which was not a given considering that some problems cannot avoid a high dependency on the size of the uncompressed string (as detailed in section~\ref{sec:intro:scalability}). However, we expect our space complexity to be far from optimal and leave improvements as well as the general case of a distance in $[a,b]$\footnote{We started writing a solution for the general case, but it was very technical and had an unreasonable space-complexity of $\tilde \Oh(g^{15})$...} as open questions.


% PM
Partially motivated by the limitations of a space usage in $\tilde \Oh(g^5)$, In Chapter~\ref{chap:gapped_pm}, we consider the dual problem: consecutive pattern matching, where the patterns and the text are processed simultaneously. Note that for an uncompressed text, consecutive pattern matching can be solved by a classic online matching, just keeping track of the most recent occurrences of $P_1$ and $P_2$ in $\Oh(|T|+|P_1|+|P_2|+\occ)$ time.
% Results
We show that an analogue complexity can be used when the text is highly compressible: all consecutive occurrences can be reported in $\Oh(g+|P_1|+|P_2|+\occ)$ time (see Theorem~\ref{th:main}) where $g$ is the size of the grammar compressed text. We then derive from this result algorithms for gapped consecutive matching (Corollary~\ref{cor:ab}) and the k-closest consecutive occurrences (Corollary~\ref{cor:topk}).
% Insights
Ganardi and Gawrychowski~\cite{DBLP:conf/soda/GanardiG22} introduced an efficient sketch for grammar compressed pattern matching: boundary information. For a given pattern $p$, the $p$-boundary information of a string $s$ stores either one $p$-substring information if $s$ occurs in $p$, or several substrings of $p$ that contain the longest prefix of $s$ that is a suffix of $p$ ($p$-prefix information) and several substrings of $p$ that contain the longest suffix of $s$ that is a prefix of $p$. The authors show how use this sketch to determine in $\Oh(g+|P|)$ whether $P$ occurs in the compressed text. We sightly extend their approach to report all occurrences crossing the boundary (Lemma~\ref{lemma:crossing}). Then by repeating a similar sketch which we call ``secondary boundary information'' and  carefully handling all cases, we obtain Theorem~\ref{th:main}.

% Squares
All previous chapters rely heavily on the Fine-Wilf periodicity lemma and its corollaries (see preliminaries~\ref{sec:prelim:FW}) to efficiently represent occurrences of a string crossing a position, thus it felt only natural to also study periodicity detection. In Chapter~\ref{chap:squares} we show how to report all runs  in optimal time in the most abstract model where they can be defined: General (unordered) alphabets where the only operation allowed is an equality test between two characters. 
We first focus on the problem of square detection, then extend our approach to square and run reporting.
% Kown results and open questions
In 1984, Main and Lorentz~\cite{Main1984} designed an $\Oh(n\log n)$ time algorithm for square detection a text $T$ of size $n$ over a general unordered alphabet. They also provided a matching lower bound for strings that have $\Omega(n)$ distinct symbols but left as an open question whether a faster algorithm was possible if one restricts the size of the alphabet.
% Lowerbounds
We start by proving that the problems requires $\Omega(n \log \sigma)$ comparison even if the size of the alphabet is known (Theorem~\ref{thm:lowerbound}). Additionally, in Theorem~\ref{thm:inapproxalph}, we show that computing any relevant approximation of the number of distinct characters requires $\Omega(n\sigma)$ operations.
% General ordered
For general ordered alphabet (where an order is given on the alphabet), Crochemore~\cite{Crochemore1986} used the $f$-factorization (related to popular Lempel-Ziv factorization) to give an algorithm for square detection running in $\Oh(n\log \sigma)$ time. Roughly speaking, the $f$-factorization and Lempel-Ziv factorization (LZ factorisation) detect repetitive fragments in the text and can be computed efficiently using a suffix tree or suffix array. However, for general unordered alphabet we show that they require $\Omega(n\sigma)$ operations to be computed (as a corollary of the lower bound on approximating the alphabet), where $\sigma$ is the number of distinct characters in $T$.
% Delta approximate factorization sketch
Instead, we introduce the $\Delta$-approximate LZ factorization which will act as a sketch capturing only the squares that are sufficiently long ($\geq 8\Delta$), as opposed to the $f$-factorization and LZ factorization that capture all squares.


% Part two

% LCS

% DTW

% XBWT
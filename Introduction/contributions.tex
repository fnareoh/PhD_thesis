 \section{Contributions}\label{intro:sec:contrib}

 \todo[inline]{Work In Progress, do not review}

So far, we presented the two core challenges at the heart of text processing: enabling relevant (and sometimes complex) queries suited to specific applications and while maintaining performances that can scale to the large volumes of input data.
%
This thesis makes theoretical and practical contributions to address both needs. 
Each contribution is presented as an independent chapter corresponding to a publication. This choice was motivated by the variety of subjects and techniques as well as how they were conceived as independent projects (with varying sets of co-authors) during the Ph.D. This section is meant to give an overview of the main contributions of the thesis and how they relate to the concept of sketches.



Part~Â \ref{part:complex_queries} focuses on a theoretical study of complex queries. 
%
% Regular expressions
We start with regular expression search in the streaming setting.
%
As mentioned previously, the streaming model considers that the regular expression $R$ that we search for and the length of the stream $n$ are given in advance and can be preprocessed, then the text $T$ arrives one character at a time. For regular expression \emph{membership} we must answer after having seen $T$ entirely whether it is recognized by $R$, while for \emph{pattern matching}, at each position $r$ we must answer whether there exist a substring $T[l..r]$ recognized by $R$.
% Talk about the lowerbounds ?
% Main contribution
In Chapter~\ref{chap:regexp}, our main contribution is to identify $d$ the number of union symbol and Kleene star in $R$ as the key parameter that allows for a space efficient streaming algorithm. We design randomized Monte Carlo algorithms (meaning the execution time is deterministic, but the algorithms can err with small probability) that solve respectively regular expression membership and pattern matching in $\Oh(d^3\polylog n)$ space and $\Oh(nd^5\polylog n)$ time per character of $T$ (Theorem~\ref{th:memb}).
% Previous uses of the parameter
This parameter had already been used by Bille and Thorup~\cite{doi:10.1137/1.9781611973075.104}\footnote{They actually consider $k$ the number of strings appearing in $R$ but $k=\Theta(d)$.} who showed algorithms (not in streaming) for answering membership and pattern matching in $\Oh(m)$ space and $\Oh(n(\frac{d\log w}{w} + \log d))$ time with $w$ the size of the machine word.
% Links to previous results in streaming
In the streaming model, this parameter was already known for the two special cases of streaming dictionary matching and don't care matching. To match a dictionary of patterns $\{P_1, P_2, ... P_d \}$, which corresponds to pattern matching for the regular expression $(P_1| P_2| ... | P_d)$, a series of results~\cite{Porat:09,DBLP:journals/talg/BreslauerG14,DBLP:conf/esa/CliffordFPSS15,DBLP:conf/esa/GolanP17,DBLP:conf/icalp/GolanKP18} lead to a randomized Monte Carlo algorithm in $\Oh(d\log m)$ space and $\Oh(\log \log |\Sigma|)$ time per character.
While don't care matching for a pattern $P_1 ? P_2 ... ? P_d$ where $P_i$, $i \in [1,d]$, are strings (possibly empty) over $\Sigma$, can be written as matching $R = P_1 (1|2|\ldots|\sigma) P_2 (1|2|\ldots|\sigma) \ldots (1|2|\ldots|\sigma) P_{d}$, and Golan, Kopelowitz, and Porat~\cite{DBLP:journals/algorithmica/GolanKP19} showed that this problem can be solved by a randomized Monte Carlo algorithm in $\Oh(d \log m)$ space and $\Oh(d+\log m)$ time per character.


% Intuitions of the ideas and how it links to sketches.
Our approach relates to sketches in how we carefully chose to store only some subset of the occurrences of the string appearing in $R$.  

% Gapped

% Indexing

% PM

% Squares

% Part two

% LCS

% DTW

% XBWT
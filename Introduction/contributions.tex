 \section{Contributions}\label{intro:sec:contrib}

 \todo[inline]{Work In Progress, do not review}

So far, we presented the two core challenges at the heart of text processing: enabling relevant (and sometimes complex) queries suited to specific applications and while maintaining performances that can scale to the large volumes of input data.
%
This thesis makes theoretical and practical contributions to address both needs. 
Each contribution is presented as an independent chapter corresponding to a publication. This choice was motivated by the variety of subjects and techniques as well as how they were conceived as independent projects (with varying sets of co-authors) during the Ph.D. This section is meant to give an overview of the main contributions of the thesis and how they relate to the concept of sketches.



Part~\ref{part:complex_queries} focuses on a theoretical study of complex queries. 
%
% Regular expressions
We start with regular expression search in the streaming setting.
%
As mentioned previously, the streaming model considers that the regular expression $R$ we are searching for and the length of the stream $n$ are given in advance and can be preprocessed beforehand, then the text $T$ stats arriving character by character. For regular expression \emph{membership} we need to determine, after having seen $T$ entirely, whether it is recognized by $R$. While for \emph{pattern matching}, we must answer, at each position $r$, whether there exist a substring $T[l..r]$ recognized by $R$.
% Talk about the lowerbounds ?
% Main contribution
In Chapter~\ref{chap:regexp}, our main contribution is to identify $d$, the number of union symbol and Kleene star in $R$, as the key parameter that enables a space efficient streaming algorithm. We design randomized Monte Carlo algorithms (meaning the execution time is deterministic, but the algorithms can err with small probability) that solve regular expression membership and pattern matching in $\Oh(d^3\polylog n)$ space and $\Oh(nd^5\polylog n)$ time per character of $T$ (Theorem~\ref{th:memb}).
% Previous uses of the parameter
This parameter had already been used by Bille and Thorup~\cite{doi:10.1137/1.9781611973075.104}\footnote{They actually consider $k$ the number of strings appearing in $R$ but $k=\Theta(d)$.} who showed algorithms (not in streaming) for answering membership and pattern matching in $\Oh(m)$ space and $\Oh(n(\frac{d\log w}{w} + \log d))$ time with $w$ the size of the machine word.
% Links to previous results in streaming
In the streaming model, this parameter was already known for the two special cases of streaming dictionary matching and don't care matching. To match a dictionary of patterns $\{P_1, P_2, ... P_d \}$ of length at most $m$, which corresponds to pattern matching for the regular expression $(P_1| P_2| ... | P_d)$, a series of results~\cite{Porat:09,DBLP:journals/talg/BreslauerG14,DBLP:conf/esa/CliffordFPSS15,DBLP:conf/esa/GolanP17,DBLP:conf/icalp/GolanKP18} lead to a randomized Monte Carlo algorithm in $\Oh(d\log m)$ space and $\Oh(\log \log |\Sigma|)$ time per character.
While don't care matching for a pattern $P_1 ? P_2 ... ? P_d$ where $P_i$, $i \in [1,d]$, are strings (possibly empty) over $\Sigma$ of total length at most $m$, can be written as matching $R = P_1 (1|2|\ldots|\sigma) P_2 (1|2|\ldots|\sigma) \ldots (1|2|\ldots|\sigma) P_{d}$, and Golan, Kopelowitz, and Porat~\cite{DBLP:journals/algorithmica/GolanKP19} showed that this problem can be solved by a randomized Monte Carlo algorithm in $\Oh(d \log m)$ space and $\Oh(d+\log m)$ time per character.

% Intuitions of the ideas and how it links to sketches.
Here is a bird's eye view of how we prove our result: we start by defining \emph{atomic strings} which are the strings obtained by just keeping the concatenation and splitting at union, Kleene star and parentheses. They only contain characters of $\Sigma$ and there are $\Theta(d)$ of them. For example, for $R= \mathrm{GAT}(\mathrm{TA}\mid \mathrm{O})(\mathrm{CAT})^*$  the set of atomic strings is $\{$GAT,TA,O,CAT$\}$.
%
The basis of our approach is to store efficiently some specific occurrences of prefixes of the atomic strings. Those stored occurrences are then linked to test if there is a “partial” match of $R$ (Definition~\ref*{def:partial_occ_regexp}). Because we store only a few occurrences over periodic regions of the text, they can be very far apart, with just a long periodic substring in between. To reconstruct a partial match, we must check if that long periodic substring corresponds to a run of the Thompson's automaton. We formulate it as finding a path of a specific weight in a multi-graph. We then efficiently solve this graph problem by translating it into a circuit using addition and convolution gates that can be evaluated in a space-efficient manner using a general framework~\cite{LokshtanovN10,Bringmann17}. Additionally, we improve that framework by removing its dependency on the Extended Riemann Hypothesis (Theorem~\ref{thm:bombieri}). 
%Sketches
Here the key innovation relating to sketches is how the specific occurrences we store are chosen. It is common in streaming algorithms to handle the aperiodic and periodic strings as two separate cases.\todo{Explain why the aperiodic case is usually simpler}
Here we choose to apply this reasoning recursively on $\Oh(\log n)$ levels through the notion of ``anchor'' position (Definition~\ref{def:anchors}). For each atomic string (or a prefix of it) and anchor, either there are just a few occurrences crossing the anchor, and we can afford to store them, or the region is periodic, and we can look at an earlier position instead. This is key for achieving the desired space complexity and to store only the necessary occurrences.


% Gapped
In Chapter~\ref{chap:gapped_index}, we begin our study of gapped consecutive matching. Recall that we are given $P_1$, $P_2$, and interval $[a,b]$ and must report all pairs of positions $(i,j)$ in $T$ such that an occurrence of $P_1$ starts at position $i$, an occurrence $P_2$ starts at position $j$, there are no occurrences of $P_1$ or $P_2$ starting in the interval $[i+1,j-1]$, and finally $j-i \in [a,b]$.
% Give specific motivations of the model ?
% Indexing
% Motivation to study indexing 
Bille et al.~\cite{bille2022gapped} introduced those queries and gave a conditional lower bound stating that for indexes of size $\tilde \Oh(|T|)$ achieving a query time faster than $\tilde \Oh(|P_1|+|P_2|+\sqrt{|T|})$ would contradict the Set Disjointness conjecture, even if $a=0$ is fixed. Additionally, they provide a non-trivial upper bound that uses $\tilde \Oh(|T|)$ space and $\tilde \Oh (|P_1| +|P_2| + |T|^{2/3}\occ^{1/3})$ time to report all $\occ$ occurrences. 
% Our contributions
We study the case where $a=0$ is fixed and the text is given as a straight-line program (SLP), and our contribution is to create an index taking space polynomial in the size of the grammar that answer such queries in optimal time up to poly log factors (Theorem~\ref{thm:close_co_occurrences}). This way, we circumvent the lower bound for highly compressible texts.
% Sketch of the idea
% PM

% Squares

% Part two

% LCS

% DTW

% XBWT
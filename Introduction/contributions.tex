 \section{Contributions}\label{intro:sec:contrib}

 \todo[inline]{If I have time to write up the streaming gapped, add a description of it, else remove it of the rest of the introduction.}

So far, we presented the two core challenges at the heart of text processing: enabling relevant (and sometimes complex) queries suited to specific applications while also maintaining performances that can scale to large volumes of data.
%
This thesis makes theoretical and practical contributions to address both needs. 
Each chapter is  an independent publication, with a specific problem, a review of the state of the art, and contribution. This choice was motivated by the variety of subjects and techniques used, and how they were conceived as independent projects (with varying sets of co-authors) during the Ph.D. However, this section intends to provide an overview of the main contributions of the thesis, some insights into the techniques used, and how they relate to the concept of sketches.

\textbf{Part~\ref{part:complex_queries}} focuses on a theoretical study of complex queries. 
%
% Regular expressions
We start with regular expression search in the streaming setting.
%
As mentioned previously, the streaming model considers that the regular expression $R$ we are searching for and the length of the stream $n$ are given in advance and can be preprocessed, then the text $T$ starts arriving character by character. For regular expression \emph{membership} we need to determine, after having seen $T$ entirely, whether it matches a regular expression $R$, while for \emph{pattern matching}, we must answer, at each position $r$, whether there exists a substring $T[l..r]$ recognized by $R$.
% Talk about the lowerbounds ?
% Main contribution
In \textbf{Chapter~\ref{chap:regexp}}, our main contribution is to identify $d$, the number of union symbols and Kleene stars in $R$, as the key parameter that enables a space-efficient streaming algorithm. We design randomized Monte Carlo algorithms (meaning the execution time is deterministic, but the algorithms can err with a small probability) that solve regular expression membership and pattern matching in $\Oh(d^3\polylog n)$ space and $\Oh(nd^5\polylog n)$ time per character of $T$ (Theorem~\ref{th:memb}).
% Previous uses of the parameter
Bille and Thorup~\cite{doi:10.1137/1.9781611973075.104}\footnote{They actually consider $k$ the number of strings appearing in $R$ but $k=\Theta(d)$.} already used this parameter to propose algorithms (not in the streaming model) to solve membership and pattern matching of a regular expression of length $m$ in $\Oh(m)$ space and $\Oh(n(\frac{d\log w}{w} + \log d))$ time, where $w$ is the size of the machine word.
% Links to previous results in streaming
In the streaming model, this parameter was already known for the two special cases of streaming dictionary matching and don't care matching. To match a dictionary of patterns $\{P_1, P_2, ... P_d \}$ of length at most $m$, which corresponds to pattern matching for the regular expression $(P_1| P_2| ... | P_d)$, a series of results~\cite{Porat:09,DBLP:journals/talg/BreslauerG14,DBLP:conf/esa/CliffordFPSS15,DBLP:conf/esa/GolanP17,DBLP:conf/icalp/GolanKP18} led to the development of a randomized Monte Carlo algorithm in $\Oh(d\log m)$ space and $\Oh(\log \log |\Sigma|)$ time per character.
Don't care pattern matching for a pattern $P_1 ? P_2 ... ? P_d$ where $P_i$, $i \in [1,d]$, are strings (possibly empty) over $\Sigma$ of total length at most $m$, can be expressed as matching $R = P_1 (1|2|\ldots|\sigma) P_2 (1|2|\ldots|\sigma) \ldots (1|2|\ldots|\sigma) P_{d}$, and Golan, Kopelowitz, and Porat~\cite{DBLP:journals/algorithmica/GolanKP19} showed that this problem can be solved by a randomized Monte Carlo algorithm in $\Oh(d \log m)$ space and $\Oh(d+\log m)$ time per character.

% Intuitions of the ideas and how it links to sketches.
Here is a bird's eye view of how we prove our result: we start by defining \emph{atomic strings} which are the strings obtained by just keeping the concatenation and splitting at union, Kleene star, and parentheses. They only contain characters of $\Sigma$ and there are $\Theta(d)$ of them. For example, for $R= \mathrm{GAT}(\mathrm{TA} | \mathrm{O})(\mathrm{CAT})^*$  the set of atomic strings is $\{$GAT, TA, O, CAT$\}$.
%
The basis of our approach is to efficiently store some specific occurrences of prefixes of the atomic strings in the text $T$. Those stored occurrences are then linked to test if there is a “partial” match of $R$ (Definition~\ref*{def:partial_occ_regexp}).
Over periodic regions of the text, there can be too many occurrences to store them all.
Thus, we chose to store only a few of those occurrences that can be very far apart, with just a long periodic substring in between. To reconstruct a partial match, we must check if that long periodic substring corresponds to a run of the Thompson automaton. We formulate it as finding a path of a specific weight in a multi-graph. We then efficiently solve this graph problem by translating it into a circuit using addition and convolution gates that can be evaluated in a space-efficient manner using a general framework~\cite{LokshtanovN10,Bringmann17}. Additionally, we improve that framework by removing its dependency on the Extended Riemann Hypothesis (Theorem~\ref{thm:bombieri}). 
%Sketches
Here the sketches are Karp--Rabin fingerprints (Preliminaries~\ref{sec:prelim:KR}) used to detect matches of the prefixes of atomic strings though it is hidden in the streaming pattern matching algorithm (Theorem~\ref{th:pattern_matching}) that we use as a black box.
% MY contrib
My personal contribution to this work has been helping to formalize and prove key properties of ``anchor'' positions (Definition~\ref{def:anchors}) which serve to efficiently choose which occurrences are stored and is key for achieving the desired space complexity. I also helped in developing the streaming algorithm, but I did not contribute to the use and improvement of the circuit framework (Section~\ref{regexp:sec:paths-in-graph}).

%Here the key innovation relating to sketches is how the specific occurrences we store are chosen. Due to properties of periodicity (see Preliminaries~\ref{sec:prelim:FW}), it is common in streaming algorithms to handle the aperiodic and periodic strings as two separate cases.
%Here we choose to apply this reasoning recursively on $\Oh(\log n)$ levels through the notion of the . For each atomic string (or a prefix of it) and anchor position, either there are just a few occurrences crossing the anchor, and we can afford to store them, or the region is periodic, and we can look at an earlier position instead. This is key for achieving the desired space complexity and to storing the necessary occurrences.


% Gapped
In \textbf{Chapter~\ref{chap:gapped_index}}, we begin our study of gapped consecutive matching. Recall that we are given $P_1$, $P_2$, and an interval $[a,b]$ and must report all pairs of positions $(i,j)$ in~$T$ such that an occurrence of $P_1$ starts at position $i$, an occurrence of $P_2$ starts at position $j$, there are no occurrences of $P_1$ or $P_2$ starting in the interval $[i+1,j-1]$, and finally $j-i \in [a,b]$.
% Indexing
% Motivation to study indexing 
Bille et al.~\cite{bille2022gapped} introduced those queries and gave a conditional lower bound stating that for indexes of size $\tilde \Oh(|T|)$ (the $\tilde \Oh$ notation hides logarithmic factors) achieving a query time faster than $\tilde \Oh(|P_1|+|P_2|+\sqrt{|T|})$ would contradict the Set Disjointness conjecture, even if $a=0$ is fixed. Additionally, they provide a non-trivial upper bound that uses $\tilde \Oh(|T|)$ space and $\tilde \Oh (|P_1| +|P_2| + |T|^{2/3}\occ^{1/3})$ time to report all $\occ$ occurrences. 
% Our contributions
We study the case where $a=0$ is fixed, and the text $T$ of size $n$ is given as a straight-line program $G$ of size $g$. A straight-line program (SLP) is a context-free grammar generating exactly one string. For example the grammar with the non-terminals $\{A,B,C,D\}$ and rules $\{A \rightarrow BC, B \rightarrow b, C \rightarrow DD, D\rightarrow d \}$ generates the string \texttt{bdd}.
We chose this formalism for its convenient interface that allows random access in $\Oh(\log n)$ time~\cite{random_access_grammar_compress} and that it captures the  Lempel--Ziv factorization up to a logarithmic factor: a Lempel--Ziv factorization of size $z$ can be transformed into an SLP of size $\Oh(z\log n)$~\cite{CharikarLLPPRSS02,Rytter02}.
%
Our contribution is to create an index taking space polynomial in the size of the grammar that answers find gapped consecutive occurrences in optimal time up to poly log factors.
To report consecutive occurrences without constraints on the distance between $P_1$ and $P_2$, our index uses $\Oh(g^2\log^4|T|)$ space where $g$ is the size of the SLP (see Corollary~\ref{cor:all}).
% Insights
This is obtained through Theorem~\ref{th:occurrences} where we provide an efficient data structure to compute several types of queries. We rely on an efficient construction of compact tries (see Preliminaries~\ref{sec:prelim:tries}) which takes advantage of the strings being prefixes and suffixes of the non-terminals. This implementation uses Karp--Rabin Fingerprints (see Preliminaries~\ref{sec:prelim:KR}) and the compacted tries are then augmented using a heavy path decomposition (see Preliminaries~\ref{sec:prelim:HP}).
Theorem~\ref{th:occurrences} is then reused for our main result: Theorem~\ref{thm:close_co_occurrences}, with an index that can report consecutive occurrences with distances in an interval $[0,b]$ using $\Oh (g^5\log^5(|T|))$ space.
% Sketches
The straight-line program input is the main sketch used in this work, but I would argue that the index we construct forms a grammar-based sketch specific to consecutive occurrences.
% Comments
The index of Theorem~\ref{thm:close_co_occurrences} circumvents the lower bound for highly compressible texts (such that $g^5 << n^2$), which is a non-trivial result since some problems cannot avoid a high dependency on the size of the uncompressed string (as detailed in Section~\ref{sec:intro:scalability}). However, we expect our space complexity to be far from optimal and leave improvements as well as the general case with $0 \leq a \leq b \leq |T|$ as open questions\footnote{We started writing a solution for the general case, but it was very technical and had an unreasonable space-complexity of $\tilde \Oh(g^{15})$...}. I contributed to every part of that work.

% PM
Partially motivated by the limitations of a space usage in $\tilde \Oh(g^5)$, in \textbf{Chapter~\ref{chap:gapped_pm}}, we address the dual problem: consecutive pattern matching, where the patterns and the text are processed simultaneously. Note that for an uncompressed text, consecutive pattern matching can be solved by a classic online matching algorithm, just keeping track of the most recent occurrences of $P_1$ and $P_2$ in $\Oh(|T|+|P_1|+|P_2|+\occ)$ time.
% Results
We show that a similar complexity can be achieved when the text is highly compressible: all consecutive occurrences can be reported in $\Oh(g+|P_1|+|P_2|+\occ)$ time (see Theorem~\ref{th:main}) where $g$ is the size of the grammar compressed text. We then derive from this result algorithms for gapped consecutive matching (Corollary~\ref{cor:ab}) and the k-closest consecutive occurrences (Corollary~\ref{cor:topk}).
% Insights
Our result is based on the efficient ``boundary information'' encoding recently introduced by Ganardi and Gawrychowski~\cite{DBLP:conf/soda/GanardiG22}. For a given pattern $p$, the $p$-boundary information of a string $s$ stores substrings occurring both in $p$ and $s$. They are chosen to capture just the information needed to detect new occurrences of $p$ that could occur when concatenating a string to $s$.
% either one $p$-substring information if $s$ occurs in $p$, or several substrings of $p$ that contain the longest prefix of $s$ that is a suffix of $p$ ($p$-prefix information) and several substrings of $p$ that contain the longest suffix of $s$ that is a prefix of $p$. 
The authors show how to use this encoding to determine in $\Oh(g+|P|)$ time whether $P$ occurs in the compressed text. We sightly extend their approach to report all occurrences crossing the boundary (Lemma~\ref{lemma:crossing}). Then we repeat this technique on a second level with ``secondary boundary information'' and carefully analyse all cases to obtain Theorem~\ref{th:main}. Here, again, the main sketch is the straight-line program we work on, and I contributed to all aspects of the publication.
%but the boundary information (and secondary boundary information) could also be seen as sketches on pattern combined to the text.

% Squares
All previous chapters rely heavily on periodicity detection to efficiently represent occurrences of a string crossing a position (see Preliminaries~\ref{sec:prelim:FW}), thus it felt only natural to also study periodicity detection. In \textbf{Chapter~\ref{chap:squares}}, we show how to report all runs  in optimal time in the most abstract model where they can be defined: General (unordered) alphabets where the only operation allowed is an equality test between two characters. 
We first focus on the problem of square detection, then extend our approach to square and run reporting.
% Kown results and open questions
In 1984, Main and Lorentz~\cite{Main1984} designed an $\Oh(n\log n)$ time algorithm for square detection in a text $T$ of size $n$ over a general unordered alphabet. They also provided a matching lower bound for strings that have $\Omega(n)$ distinct symbols but left as an open question whether a faster algorithm was possible if the size of the alphabet $\sigma=|\Sigma|$ is restricted.
% Lowerbounds
We start by proving that the problem requires $\Omega(n \log \sigma)$ comparison even if the size of the alphabet is known (Theorem~\ref{thm:lowerbound}). Additionally, in Theorem~\ref{thm:inapproxalph}, we show that computing any relevant approximation of the number of distinct characters requires $\Omega(n\sigma)$ operations.
% General ordered
For general ordered alphabets (where an order is given), Crochemore~\cite{Crochemore1986} used the $f$-factorization (related to popular Lempel--Ziv factorization) to give an algorithm for square detection running in $\Oh(n\log \sigma)$ time. Roughly speaking, the $f$-factorization and Lempel--Ziv factorization (LZ factorization) detect repetitive fragments in the text and can be computed efficiently using a suffix tree or suffix array. However,  we show that on general unordered alphabets, those factorizations require $\Omega(n\sigma)$ operations to be computed (as a corollary of the lower bound on approximating the alphabet). %, where $\sigma$ is the number of distinct characters in $T$.
% Delta approximate factorization sketch
Instead, we introduce the $\Delta$-approximate LZ factorization which acts as a sketch capturing only the sufficiently long squares (of length at least $8\Delta$), as opposed to the $f$-factorization and LZ factorization that capture all squares.
We present our final algorithm in steps. We first assume that the alphabet size is known and focus on giving an upper bound on the number of comparisons, then we proceed to remove the assumption on knowing the alphabet size, and finally we provide an overall efficient algorithm running in time $\Oh(n\log\sigma)$.
% MY contrib
In this work, I helped formalize and clarify the proofs throughout the process. I give to my co-authors Jonas Ellert and Pawel Gawrychowski the credit for the major technical ideas.

% Part two
\textbf{Part~\ref{part:approx-bio}} explores the use of approximation to further reduce the size of sketches and provide more efficient algorithms. Each chapter provides a proof of concept implementation for bioinformatics applications.
%
% LCS approx k
Earlier in this introduction, we detailed the importance of similarity measures for numerous applications.
% ED
In bioinformatics the edit distance is the most popular similarity measure, however, Backurs and Indyk~\cite{DBLP:conf/stoc/BackursI15} proved a conditional lower bound (based on SETH) which suggests it is unlikely to be computable in strongly subquadratic time.
% ED approx
The need to overcome this quadratic-time barrier led to the study of approximate algorithms for the edit distance. Chakraborty et al.~\cite{DBLP:conf/focs/ChakrabortyDGKS18} gave the first breakthrough result with a constant-factor approximation algorithm that computes the edit distance between two strings of length $n$ in time $\tilde{\Oh}(n^{2-2/7})$.
Since our publication~\cite{DBLP:conf/cpm/GourdelKRS20} (presented in Chapter~\ref{chap:LCS}), a series of works have been published on approximating the edit distance~\cite{brakensiek2020constant,koucky2020constant} with the strongest result being~\cite{andoni2020edit} with a constant factor approximation in time $n^{1+\eps}$ for any $\eps>0$ (where the approximation constant depends solely on $\eps$).
Nevertheless, these algorithms tend to be quite technical and even those meant to be simpler such as~\cite{andoni2020simple} do not seem to have been implemented and evaluated in practice.
% Our problem 
In \textbf{Chapter~\ref{chap:LCS}}, we take another approach by considering a different similarity measure meant to be both robust to small changes and simple enough to allow for efficient computation. We consider \kApproxLCS which is an approximate version of \kLCS. The formal definition of the latter is, for an integer $k$ and two strings $X$ and $Y$, $\lcsk(X,Y)$ is the maximal length of a substring of $X$ that occurs in $Y$ with at most $k$ mismatches.\todo{This will be presented in the similarity measure paragraph instead}
For a constant $\eps > 0$, the \kApproxLCS problem needs to return a substring of $X$ of length at least $\lcsk(X,Y)$ that occurs in $Y$ with at most $(1+\eps) \cdot k$ mismatches. This problem has been introduced by Kociumaka, Radoszewski, and Starikovskaya~\cite{DBLP:journals/algorithmica/KociumakaRS19} after they showed that there is $k=\Theta(\log n)$ such that \kLCS cannot be solved in strongly subquadratic time.
% Our contrib
In Theorem~\ref{th:klcs_upper} we provide two algorithms: one assuming a constant size alphabet running in $\Oh(n^{1+ 1/(1+2\eps) + o(1)})$ time and space, and one in $\Oh(n^{1+1/(1+\eps)} \log^3 n)$ time and linear space without constraints on the alphabet. The first result relies on an Approximate Nearest Neighbour data structure~\cite{DBLP:conf/stoc/AndoniR15} as a black box and is more of theoretical interest. In contrast, our second contribution is simpler and more practical as we confirm by an experimental evaluation.
Additionally, in Fact~\ref{lm:klcs_lower}, we show a conditional lower bound for \kApproxLCS (with a construction similar to the proof for the lower bound of \kLCS ).
% Sketches
In our algorithms, we rely on Karp--Rabin fingerprints and a sketch estimating the Hamming distance based on dimension reduction, both detailed in Section~\ref{lcs:sec:prelim}.
% MY contrib
I participated in all aspects of the work and implemented the practical evaluation of our algorithm.

% DTW
Continuing our exploration of similarity measures and distances, the next chapter focuses on the Dynamic Time Warping (DTW) distance. Recall that for the DTW distance, one must ``wrap'' the two strings: double some character until the strings are of equal length and then sum the distances between the characters at the same positions.
% RLE / Sketches
In \textbf{Chapter~\ref{chap:DTW}}, we consider one of the simplest from of sketching: run-length encoding. The run-length encoding of a string $S$ of length $N$ is a concise representation $\RLE(S)=(c_1,l_1)(c_2,l_2)...(c_n,l_n)$ where $(c_i,l_i)$ represents the character $c_i \in \Sigma$ repeated $l_i$ times  for $i \in [1,n]$, and such that $\sum_{i\in [1,n]} l_i = N$.
This sketch is especially relevant for DTW as the runs of equal characters tend to be aligned despite variations in length. That is why the number of runs in the strings have already been used by Froese et al.~\cite{DBLP:journals/corr/abs-1903-03003} who gave an algorithm computing the DTW distance between two string with running time $\Oh(mN+nM)$, where $M,N$ are the length of the strings, and $m, n$ are the sizes of their run length encodings.

% Our contrib 
Our contribution is as follows: when the distances between characters of $\Sigma$ are integers, for a pattern $P$ with $m$ runs and a text $T$ with $n$ runs, we show that there is $\Oh(n+m)$-time algorithm that computes all locations $j$ where the DTW distance between $P$ and a suffix of $T[..j]$ is at most 1. More generally for an integer $k$, we provide an algorithm $\Oh(knm)$-time that computes all locations $j$ where the DTW distance between $P$ and a suffix of $T[..j]$ is at most $k$.
% Bio motivation
Our interest and research on DTW is also motivated by potential applications to analysing biological data produced by the third generation sequencing which we detail in Section~\ref{dtw:sec:experiments}.
% My contrib
I contributed to every part of that work except for the approximation algorithms (given as corollary of our second algorithm) which were added by Tatiana Starikovskaya.
% Update
Since our publication, Boneh, Golan, Mozes, and Weimann uploaded a preprint~\cite{boneh2023near} with a $\tilde{\Oh}(n^2)$-time computation of the DTW distance between two strings of length $n$, which is optimal up to log factors. They follow the approach that Clifford et al.~\cite{clifford2019rle} used to show a similar result for the edit distance: they represent and manipulate the inputs and outputs with a piecewise-linear function.

% XBWT
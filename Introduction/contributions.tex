 \section{Contributions}\label{intro:sec:contrib}

 \todo[inline]{If I have time to write up the streaming gapped, add a description of it, else remove it of the rest of the introduction.\\ Check past/present tense inconsistency when describing previous work.}

So far, we presented the two core challenges at the heart of modern text processing: enabling relevant (and sometimes complex) queries suited to specific applications while also maintaining performances that can scale to large volumes of data.
%
This thesis makes theoretical and practical contributions to address both needs. 
Each chapter is  an independent publication, with a specific problem, a review of the state of the art, and a set of contributions. This choice was motivated by the variety of subjects and techniques used, and how they were conceived as independent projects (with varying sets of co-authors) during the Ph.D. However, this section intends to provide an overview of the main contributions of the thesis, some insights into the techniques used, and how they relate to the concept of sketches.


\textbf{Part~\ref{part:complex_queries}} focuses on a theoretical study of complex queries. 
%
% Regular expressions
We start with regular expression search in the streaming setting.
%
We assume to be given a regular expression $R$, a streaming text $T$ of length $n$, and must output, for regular expression \emph{membership} we need to determine, after having seen $T$ entirely, whether it matches a regular expression $R$, while for \emph{pattern matching}, we must answer, at each position $r$, whether there exists a substring $T[l..r]$ recognized by $R$.
% Talk about the lowerbounds ?
% Main contribution
In \textbf{Chapter~\ref{chap:regexp}}, our main contribution is to identify $d$, the number of union symbols and Kleene stars in $R$, as the key parameter that enables a space-efficient streaming algorithm. We design randomized Monte Carlo algorithms (meaning the execution time is deterministic, but the algorithms can err with a small probability) that solve regular expression membership and pattern matching in $\Oh(d^3\polylog n)$ space and $\Oh(nd^5\polylog n)$ time per character of $T$ (Theorem~\ref{th:memb}).
% Previous uses of the parameter
Bille and Thorup~\cite{doi:10.1137/1.9781611973075.104}\footnote{They actually consider $k$ the number of strings appearing in $R$ but $k=\Theta(d)$.} already used this parameter to propose algorithms to solve membership and pattern matching of a regular expression of length $m$ in $\Oh(m)$ space and $\Oh(n(\frac{d\log w}{w} + \log d))$ time, where $w$ is the size of the machine word.
% Links to previous results in streaming
In the streaming model, this parameter was already known for the two special cases of streaming dictionary matching and don't care matching. In dictionary matching, we search for occurrences of a set of patterns $\{P_1, P_2, ... P_d \}$ of total length at most $m$, which corresponds to pattern matching for the regular expression $(P_1| P_2| ... | P_d)$, a series of results~\cite{Porat:09,DBLP:journals/talg/BreslauerG14,DBLP:conf/esa/CliffordFPSS15,DBLP:conf/esa/GolanP17,DBLP:conf/icalp/GolanKP18} led to the development of a randomized Monte Carlo algorithm in $\Oh(d\log m)$ space and $\Oh(\log \log |\Sigma|)$ time per character.
Don't care pattern matching for a pattern $P_1 ? P_2 ... ? P_d$ where $P_i$, $i \in [1,d]$, are strings (possibly empty) over $\Sigma$ of total length at most $m$, can be expressed as matching $R = P_1 (1|2|\ldots|\sigma) P_2 (1|2|\ldots|\sigma) \ldots (1|2|\ldots|\sigma) P_{d}$, and Golan, Kopelowitz, and Porat~\cite{DBLP:journals/algorithmica/GolanKP19} showed that this problem can be solved by a randomized Monte Carlo algorithm in $\Oh(d \log m)$ space and $\Oh(d+\log m)$ time per character.
\todo[inline]{If possible try to improve the transitions in the paragraph above.}

% Intuitions of the ideas and how it links to sketches.
Here is a bird's eye view of how we prove our result: we start by defining \emph{atomic strings} which are the ``words'' appearing in the regular expression. They only contain characters of $\Sigma$ and there are $\Theta(d)$ of them. For example, for $R= \mathrm{GAT}(\mathrm{TA} | \mathrm{O})(\mathrm{CAT})^*$  the set of atomic strings is $\{$GAT, TA, O, CAT$\}$.
%
The basis of our approach is to efficiently store some specific occurrences of prefixes of the atomic strings in the text $T$. Those stored occurrences are then linked to test if there is a “partial” match of $R$ (Definition~\ref*{def:partial_occ_regexp}).
Over periodic regions of the text, there can be too many occurrences to store them all.
Thus, we chose to store only a few of those occurrences that can be very far apart, with just a long periodic substring in between. To reconstruct a partial match, we must check if that long periodic substring corresponds to a run of the Thompson automaton. We formulate it as finding a path of a specific weight in a multi-graph. We then efficiently solve this graph problem by translating it into a circuit using addition and convolution gates that can be evaluated in a space-efficient manner using a general framework~\cite{LokshtanovN10,Bringmann17}. Additionally, we improve that framework by removing its dependency on the Extended Riemann Hypothesis (Theorem~\ref{thm:bombieri}). 
%Sketches
Here the sketches are Karp--Rabin fingerprints (Preliminaries~\ref{sec:prelim:KR}) used to detect matches of the prefixes of atomic strings though it is hidden in the streaming pattern matching algorithm (Theorem~\ref{th:pattern_matching}) that we use as a black box.
% MY contrib
My personal contribution to this work was focussed on formalizing and proving key properties of ``anchor'' positions (Definition~\ref{def:anchors}) which serve to efficiently choose which occurrences are stored and is key for achieving the desired space complexity. I also participated in developing the streaming algorithm. %but I did not contribute to the use and improvement of the circuit framework (Section~\ref{regexp:sec:paths-in-graph}).

%Here the key innovation relating to sketches is how the specific occurrences we store are chosen. Due to properties of periodicity (see Preliminaries~\ref{sec:prelim:FW}), it is common in streaming algorithms to handle the aperiodic and periodic strings as two separate cases.
%Here we choose to apply this reasoning recursively on $\Oh(\log n)$ levels through the notion of the . For each atomic string (or a prefix of it) and anchor position, either there are just a few occurrences crossing the anchor, and we can afford to store them, or the region is periodic, and we can look at an earlier position instead. This is key for achieving the desired space complexity and to storing the necessary occurrences.


% Gapped
In \textbf{Chapter~\ref{chap:gapped_index}}, we begin our study of gapped consecutive matching. In this problem, we are given patterns $P_1$, $P_2$, and an interval $[a,b]$ and must report all pairs of positions $(i,j)$ in a text $T$ such that an occurrence of $P_1$ starts at position $i$, an occurrence of $P_2$ starts at position $j$, there are no occurrences of $P_1$ or $P_2$ starting in the interval $[i+1,j-1]$, and finally $j-i \in [a,b]$.
% Indexing
% Motivation to study indexing 
Bille et al.~\cite{bille2022gapped} introduced those queries and gave a conditional lower bound stating that for indexes of size $\tilde \Oh(|T|)$ (the $\tilde \Oh$ notation hides polylogarithmic factors) achieving a query time faster than $\tilde \Oh(|P_1|+|P_2|+\sqrt{|T|})$ would contradict the Set Disjointness conjecture, even if $a=0$ is fixed. Additionally, they provided a non-trivial upper bound that uses $\tilde \Oh(|T|)$ space and $\tilde \Oh (|P_1| +|P_2| + |T|^{2/3}\occ^{1/3})$ time to report all $\occ$ occurrences. 
% Our contributions
We assume $a=0$ is fixed, and that the text $T$ of size $n$ is given as a straight-line program $G$ of size $g$, which is a context-free grammar generating exactly one string. For example the grammar with the non-terminals $\{A,B,C,D\}$ and rules $\{A \rightarrow BC, B \rightarrow b, C \rightarrow DD, D\rightarrow d \}$ generates the string \texttt{bdd}.
We chose this formalism 
%for its convenient interface that allows random access in $\Oh(\log n)$ time~\cite{random_access_grammar_compress} 
as it captures the popular Lempel--Ziv factorization up to a logarithmic factor: a Lempel--Ziv factorization of size $z$ can be transformed into an SLP of size $\Oh(z\log n)$~\cite{CharikarLLPPRSS02,Rytter02}.
%
Our contribution is to create an index taking space polynomial in the size of the grammar that reports consecutive occurrences with distances in $[0,b]$ in optimal time up to poly log factors.
To report consecutive occurrences without constraints on the distance between $P_1$ and $P_2$, our index uses $\Oh(g^2\log^4|T|)$ space where $g$ is the size of the SLP (see Corollary~\ref{cor:all}).
% Insights
%This is obtained through Theorem~\ref{th:occurrences} where we provide an efficient data structure to compute several types of queries. 
We rely on an efficient construction of compact tries (see Preliminaries~\ref{sec:prelim:tries}) which takes advantage of the strings being prefixes and suffixes of strings generated by non-terminals. This implementation uses Karp--Rabin fingerprints (see Preliminaries~\ref{sec:prelim:KR}) and the compacted tries are then augmented using a heavy path decomposition (see Preliminaries~\ref{sec:prelim:HP}).
We then reuse this structure for our main result: Theorem~\ref{thm:close_co_occurrences}, with an index that can report consecutive occurrences with distances in an interval $[0,b]$ using $\Oh (g^5\log^5(|T|))$ space.
% Sketches
The straight-line program input is the main sketch used in this work, but I would argue that the index we construct forms a grammar-based sketch specific to consecutive occurrences.
% Comments
The index of Theorem~\ref{thm:close_co_occurrences} circumvents the lower bound for highly compressible texts (such that $g^5 << n^2$), which is a non-trivial result since some problems cannot avoid a high dependency on the size of the uncompressed string (as detailed in Section~\ref{sec:intro:scalability}). However, we expect our space complexity to be far from optimal and leave improvements as well as the general case with $0 \leq a \leq b \leq |T|$ as open questions\footnote{We started writing a solution for the general case, but it was very technical and had an unreasonable space-complexity of $\tilde \Oh(g^{15})$...}. I contributed to every part.


% PM
Partially motivated by the limitations of a space usage in $\tilde \Oh(g^5)$, in \textbf{Chapter~\ref{chap:gapped_pm}}, we address the dual problem: consecutive pattern matching, where the patterns and the text are processed simultaneously. Note that for an uncompressed text, consecutive pattern matching can be solved by a classic online matching algorithm, just keeping track of the most recent occurrences of $P_1$ and $P_2$ in $\Oh(|T|+|P_1|+|P_2|+\occ)$ time.
% Results
We show that a similar complexity can be achieved when the text is highly compressible: all consecutive occurrences can be reported in $\Oh(g+|P_1|+|P_2|+\occ)$ time (see Theorem~\ref{th:main}) where $g$ is the size of the grammar compressed text. We then derive from this result algorithms for gapped consecutive matching (Corollary~\ref{cor:ab}) and the k-closest consecutive occurrences (Corollary~\ref{cor:topk}).
% Insights
Our result is based on the efficient ``boundary information'' encoding recently introduced by Ganardi and Gawrychowski~\cite{DBLP:conf/soda/GanardiG22}. For a given pattern $P$, the $P$-boundary information of a string $S$ stores substrings occurring both in $P$ and $S$. They are chosen to capture just the information needed to detect new occurrences of $P$ that could occur when concatenating a string to $S$.
% either one $P$-substring information if $S$ occurs in $P$, or several substrings of $P$ that contain the longest prefix of $S$ that is a suffix of $P$ ($P$-prefix information) and several substrings of $P$ that contain the longest suffix of $S$ that is a prefix of $P$. 
The authors show how to use this encoding to determine in $\Oh(g+|P|)$ time whether $P$ occurs in the compressed text. We sightly extend their approach to report all occurrences crossing the boundary (Lemma~\ref{lemma:crossing}). Then we repeat this technique on a second level with ``secondary boundary information'' and carefully analyse all cases to obtain Theorem~\ref{th:main}. Here, again, the main sketch is the straight-line program we work on, and I contributed to all aspects of the publication.
%but the boundary information (and secondary boundary information) could also be seen as sketches on pattern combined to the text.


% Squares
Intrinsically, all previous chapters rely heavily on periodicity detection, thus it felt only natural to study this problem in \textbf{Chapter~\ref{chap:squares}}. We show how to report all runs  in optimal time in the most abstract model where they can be defined: General (unordered) alphabets where the only operation allowed is an equality test between two characters. 
We first consider the problem of square detection, then extend our approach to square and run reporting.
% Kown results and open questions
In 1984, Main and Lorentz~\cite{Main1984} designed an $\Oh(n\log n)$ time algorithm for square detection in a text $T$ of size $n$ over a general unordered alphabet. They also provided a matching lower bound for strings that have $\Omega(n)$ distinct symbols but left as an open question whether a faster algorithm was possible if the size of the alphabet $\sigma=|\Sigma|$ is restricted.
% Lowerbounds
We start by proving that the problem requires $\Omega(n \log \sigma)$ comparisons even if the size of the alphabet is known (Theorem~\ref{thm:lowerbound}). Additionally, in Theorem~\ref{thm:inapproxalph}, we show that computing any relevant approximation of the number of distinct characters requires $\Omega(n\sigma)$ operations.
% General ordered
For general ordered alphabets (where an order is given), Crochemore~\cite{Crochemore1986} used the $f$-factorization (related to popular Lempel--Ziv factorization) to give an algorithm for square detection running in $\Oh(n\log \sigma)$ time. Roughly speaking, the $f$-factorization and Lempel--Ziv factorization (LZ-factorization) detect repetitive fragments in the text and can be computed efficiently using a suffix tree or suffix array. However,  we show that on general unordered alphabets, those factorizations require $\Omega(n\sigma)$ operations to be computed (as a corollary of the lower bound on approximating the alphabet). %, where $\sigma$ is the number of distinct characters in $T$.
% Delta approximate factorization sketch
Instead, we introduce the $\Delta$-approximate LZ-factorization which acts as a sketch capturing only sufficiently long squares (of length at least $8\Delta$), as opposed to the $f$-factorization and the LZ-factorization that capture all squares.
We present our final algorithm in steps. We first assume that the alphabet size is known and focus on giving an upper bound on the number of comparisons, then we proceed to remove the assumption on knowing the alphabet size, and finally we provide an overall efficient algorithm running in time $\Oh(n\log\sigma)$.
% MY contrib
In this work, I participated in formalizing and clarifying the proofs throughout the research process. %I give to my co-authors Jonas Ellert and Pawel Gawrychowski the credit for the major technical ideas.

% Part two
\textbf{Part~\ref{part:approx-bio}} explores the use of approximation to further reduce the size of sketches and provide more efficient algorithms. Each chapter provides a proof of concept implementation for bioinformatics applications.
%
% LCS approx k
Earlier in this introduction, we detailed the importance of similarity measures for numerous applications.
% ED
In bioinformatics, the edit distance is arguably the most popular similarity measure, however, Backurs and Indyk~\cite{DBLP:conf/stoc/BackursI15} proved a conditional lower bound (based on SETH) which suggests it is unlikely to be computable in strongly subquadratic time.
% ED approx
The need to overcome this quadratic-time barrier led to the study of approximate algorithms for the edit distance. Chakraborty et al.~\cite{DBLP:conf/focs/ChakrabortyDGKS18} gave the first breakthrough result with a constant-factor approximation algorithm that computes the edit distance between two strings of length $n$ in time $\tilde{\Oh}(n^{2-2/7})$.
Since our publication~\cite{DBLP:conf/cpm/GourdelKRS20} (presented in Chapter~\ref{chap:LCS}), a series of works have been published on approximating the edit distance~\cite{brakensiek2020constant,koucky2020constant} with the strongest result being~\cite{andoni2020edit} with a constant factor approximation in time $n^{1+\eps}$ for any $\eps>0$ (where the approximation constant depends solely on $\eps$).
Nevertheless, these algorithms tend to be quite technical and even those meant to be simpler such as~\cite{andoni2020simple} do not seem to have been implemented and evaluated in practice.
% Our problem 
In \textbf{Chapter~\ref{chap:LCS}}, we take another approach by considering a different similarity measure meant to be both robust to small changes and simple enough to allow for efficient computation. We consider the Longest Common Substring (abbreviated LCS here after) with Approximately k mismatches which is an approximate version of \kLCS. Recall that the formal definition of the latter is, for an integer $k$ and two strings $X$ and $Y$, $\lcsk(X,Y)$ is the maximal length of a substring of $X$ that occurs in $Y$ with at most $k$ mismatches.
For a constant $\eps > 0$, the \kApproxLCS problem needs to return a substring of $X$ of length at least $\lcsk(X,Y)$ that occurs in $Y$ with at most $(1+\eps) \cdot k$ mismatches. This problem has been introduced by Kociumaka, Radoszewski, and Starikovskaya~\cite{DBLP:journals/algorithmica/KociumakaRS19} after they showed that there is $k=\Theta(\log n)$ such that \kLCS cannot be solved exactly in strongly subquadratic time (conditioned on SETH).
% Our contrib
In Theorem~\ref{th:klcs_upper}, we provide two algorithms: one assuming a constant size alphabet running in $\Oh(n^{1+ 1/(1+2\eps) + o(1)})$ time and space, and one in $\Oh(n^{1+1/(1+\eps)} \log^3 n)$ time and linear space without constraints on the alphabet. The first result relies on an Approximate Nearest Neighbour data structure~\cite{DBLP:conf/stoc/AndoniR15} as a black box, and we do not evaluate it in practice. In contrast, our second contribution is simpler, and we confirm its practicality by an experimental evaluation.
Additionally, in Fact~\ref{lm:klcs_lower}, we show a conditional lower bound for \kApproxLCS (with a construction similar to the proof for the lower bound of \kLCS ).
% Sketches
In our algorithms, we rely on Karp--Rabin fingerprints and a sketch estimating the Hamming distance based on dimension reduction, both detailed in Section~\ref{lcs:sec:prelim}.
% MY contrib
I participated in all aspects of the work and implemented the practical evaluation of our algorithm.


% DTW
Continuing our exploration of similarity measures and distances, the next chapter focuses on the Dynamic Time Warping (DTW) distance. Recall that for the DTW distance, one must ``wrap'' the two strings: double some character until the strings are of equal length and then sum the distances between the characters at the same positions.
% RLE / Sketches
In \textbf{Chapter~\ref{chap:DTW}}, we consider one of the simplest from of sketching: run-length encoding. The run-length encoding of a string $S$ of length $N$ is a concise representation $\RLE(S)=(c_1,l_1)(c_2,l_2)...(c_n,l_n)$ where $(c_i,l_i)$ represents the character $c_i \in \Sigma$ repeated $l_i$ times  for $i \in [1,n]$, and such that $\sum_{i\in [1,n]} l_i = N$.
This sketch is especially relevant for DTW as the runs of equal characters tend to be aligned despite variations in length. That is why the number of runs in the strings have already been used by Froese et al.~\cite{DBLP:journals/corr/abs-1903-03003} who gave an algorithm computing the DTW distance between two string with running time $\Oh(mN+nM)$, where $M,N$ are the length of the strings, and $m, n$ are the sizes of their run length encodings.

% Our contrib 
Our contribution is as follows: when the distances between characters of $\Sigma$ are integers, for a pattern $P$ with $m$ runs and a text $T$ with $n$ runs, we show that there is $\Oh(n+m)$-time algorithm that computes all locations $j$ where the DTW distance between $P$ and a suffix of $T[..j]$ is at most 1. More generally for an integer $k$, we provide an algorithm $\Oh(knm)$-time that computes all locations $j$ where the DTW distance between $P$ and a suffix of $T[..j]$ is at most $k$.
% Bio motivation
Our interest and research on DTW is also motivated by potential applications to analysing biological data produced by the third generation sequencing which we detail in Section~\ref{dtw:sec:experiments}.
% My contrib
I contributed to every part of that work except for the approximation algorithms (given as corollary of our second algorithm) which were added by Tatiana Starikovskaya.
% Update
Since our publication, Boneh, Golan, Mozes, and Weimann uploaded a preprint~\cite{boneh2023near} with a $\tilde{\Oh}(n^2)$-time computation of the DTW distance between two strings of length $n$, which is optimal up to log factors. They follow the approach that Clifford et al.~\cite{clifford2019rle} used to show a similar result for the edit distance: they represent and manipulate the inputs and outputs with a piecewise-linear function.



% XBWT
For \textbf{Chapter~\ref{chap:XBWT}}, we shift our focus from the study of similarity measures to addressing the practical problem of readsets indexing. 
% Approximation
Additionally, in this chapter, the perspective on approximation is different, instead of studying approximate matching where one reports all substrings at a given distance from the pattern, we propose a compressed index that has the downside of reporting false positives: occurrences of the pattern that are not fully included in a read.

% Add context on readsets and how big their representation can be despite the repetitiveness
Recall that a read is a sequence of base pairs, generally short (the precise length depends on the sequencing technique), obtained when sequencing DNA. To be able to re-assemble the entire DNA sequence, each position of the sequence is generally covered by multiple reads. The average number of reads covering a position is called the sequencing coverage.
The standard sequencing coverage for short reads is now between $30$ and $50$, making the readsets highly repetitive, especially when the DNA sequenced comes from a single individual.
%\todo[inline]{A part of this description of reads should probably be in the intro}
% BWT and importance of the FM-index for bio tools
To index a single repetitive string, the FM-index~\cite{ferragina2005indexing} based on the Burrows-Wheeler transform (BWT)~\cite{burrows1994block} is one of the most important data structures\footnote{The authors of both the BWT and FM-index received in 2022 the \textit{ACM Paris Kanellakis Theory and Practice Award} for those breakthroughs.}, and it has been applied in several bioinformatics tools for short read alignment~\cite{langmead2009ultrafast,langmead2012fast,li2009fast}.

% The issue of generalizing the BWT to a collection of strings 
Unfortunately, extending the BWT to a collection of strings is not straightforward. To better understand the context of Chapter~\ref{chap:XBWT}, let me give an overview of the current solutions and their challenges. A possible approach is to concatenate all strings of the collection with a distinct separator character between them and then build the FM-index of the resulting string. However, with this approach the string can be very large, making the BWT challenging to build and update, and one character is added to the alphabet per strings in the collection which becomes costly for large collections. Some methods avoid this increase in the alphabet size by having a single separator symbol and comparing them using their poistion in the concatenated string (this is done in tools such as \texttt{BEETL}~\cite{Beetl} and \texttt{RopeBWT}~\cite{ropebwt2}). \texttt{BigBWT}~\cite{boucher2019prefix} uses another method, there are two separator symbols: one is put between the strings and the other just at the end of the concatenation as a way to break ties. But both of those approaches create a result where two different letters followed by the same suffix (up to a dollar) will be ordered depending on the input order of their respective strings in the collection. A common heuristic (used by {BEETL}~\cite{Beetl} and \texttt{RopeBWT}~\cite{ropebwt2}) is to sort the strings in the collection according to their colexicographic order so that the strings with similar suffixes are grouped together.
% EBWT
Mantaci et al.~\cite{mantaci2007extension} generalized the concept of the BWT to collections of strings. The EBWT is a permutation of characters in the strings of a collection according to the lexicographic order of the suffixes that immediately follow those characters, considering each string as cyclic\footnote{This is the Omega order ($\preccurlyeq_\omega$) order define by Mantaci et al.~\cite{mantaci2007extension}.}.
%The EBWT is a permutation with as much cycle as there are strings in the collection compared to just one cycle for the BWT.
In the literature, several variants of the EBWT are commonly used without much distinction. The first variant is the EBWT of the collection where each string is appended a shared end-of-string symbol. The second variant is the EBWT of the collection where each string is appended a unique end-of-string symbol. 
\todo[inline]{Add more precise references and explain what are the impact of those variants}

Those nuances between definitions of the BWT for a concatenation of strings and EBWT with or without distinct end-of-string symbols may seem minor, but they do impact the transformed string, and they have become crucial in light of a recent improvement of the FM-index: the \emph{$r$-index}.
% r-index
Gagie et al.~\cite{gagie2020fully} designed an FM-index that takes space proportional to $r$, the number of runs in the BWT of the indexed string. More precisely the data structure occupies $\Oh(r\log\log n)$ space while still being able to count and locate all occurrences of a pattern in optimal time.
\todo[inline]{Add earlier the bounds for the FM-index + query time}
Since then, the parameter $r$ has been considered in several data structures and is often seen as a measure of the repetitiveness of the text.
% the variety of EBWT versions
Cenzato and Lipták~\cite{cenzato_et_al_BWT_Collections} showed in a very interesting survey of the BWT variants for string collections, that the final  number of runs can drastically change depending on the variant. 
They compared the number of runs produced by each variant to the number of runs in the BWT where the end-of-string symbols are ordered by the Bentley et al.~\cite{bentley2019complexity} order (implemented by~\cite{cenzato2023computing}), which minimize the numbers of runs in the resulting BWT.
Their experiments demonstrate the efficiency of the heuristic ordering the end-of-string symbols according to the colexicographic order of the strings in practice (which is also visible in Chapter~\ref{chap:XBWT}) and they also provide a theoretical upper bound.

% Our idea
We began the work presented in Chapter~\ref{chap:XBWT} before the survey of Cenzato and Lipták~\cite{cenzato_et_al_BWT_Collections} and the construction of an optimal order by Bentley et al.~\cite{bentley2019complexity}, and we take a different approach to optimizing the number of runs for the BWT. We propose to take advantage of information very commonly associated with reads: alignment to a reference genome. We build a tree where the main trunk is the reference and reads branch out of the trunk at the positions they are aligned to, and then compute the generalization of the BWT for trees: the eXtended Burrows--Wheeler transform (XBWT)~\cite{ferragina2009compressing}. Intuitively, this provides a context to the reads that allows for better sorting and limits the number of runs breaking in the transformed string. Formally, we show that if the reads are almost identical to the reference and that the differences are located towards the end of the reads (which is the case in practice for short reads), we can bound the number of runs in the XBWT depending on the number of runs in the BWT of the reference. We then test our approach in practice and show that it does obtain fewer runs than the BWT with the colexicographic heuristic.
% False Possitive
The main drawback of our index is that we search in the branching tree thus when counting the number of occurrences of a pattern we cannot avoid counting occurrences that are not fully contained in a read (that are partially or entirely in the main truck reference).

% The prefix free parsing construction of the XBWT
Another contribution of our work is that, with the goal of a scalable construction of the index in mind, we adapt the technique of a prefix-free parsing construction of the BWT of Boucher et al.~\cite{boucher2019prefix} to the construction of the XBWT. 
% sketches in the prefix free parsing
In the prefix-free parsing construction of the BWT, the input string is parsed into overlapping phrases as follows: we maintain the hash of a sliding window using Karp--Rabin fingerprints, and whenever the hash is equal to zero we end the current phrase and begin a new one. This technique, called context-triggered piecewise hashing, allows us to create a set of phrases that are prefix-free.
The string is then described using a \emph{dictionary} and a \emph{parse}. 
The dictionary associates each phrase with a metacharacter, and the parse is a string of metacharacters in the same order as the corresponding phrases occur in the input string.
This construction uses sketches (Karp--Rabin fingerprints) but also creates a sketch through this compact representation of the string.
The intuition is that repeated sections of the string will translate into repeated phrases that can be represented by the same metacharacters in the parse, allowing the dictionary to remain reasonably small.
The BWT or the input string is then constructed starting from the BWT of the parse and the BWTs of the phrases.
% My contributions
Adapting this technique to labelled trees and the construction of the XBWT is non-trivial because each branch may create new phrases. My contribution was focused on this adaptation, the implementation, and the experimental evaluation of our proposed data structure in terms of the number of runs in the XBWT. The implementation of the queries and analysis of false positives are left as future work.


I hope this overview of the contributions has highlighted clearly that sketching is a valuable tool in a variety of settings and applications. I expect that they will continue to appear in many future works both in theory and in practice.
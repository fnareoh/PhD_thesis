\chapter*{Introduction}\label{chap:intro}\setcounter{page}{1}\frontmatter
\addcontentsline{toc}{chapter}{Introduction}
\chaptermark{Introduction}

When answering the classic question ``What is your Ph.D. about?'' to family and friends, I always start with the ``Ctrl + F'' function in their favorite text editor or web browser. This quickly highlights one of the applications of the exact pattern-matching problem. If I feel especially ambitious in my explanations, I will attempt to give the intuition of the naive $\Oh(nm)$ algorithm. Picture a young child, aligning the string against every position of the text and comparing character by character because the child has yet to learn how to read. To show a glimpse of a more complex solution, I comment on how, depending on the pattern, the child may try to skip portions of the text. But even my grandparents immediately know that efficient search in a text has been possible for decades and that it cannot be my real research subject.

\section{Context}

Indeed, exact pattern matching has been long studied, with in particular the famous Knuth-Morris-Pratt algorithm\footnote{The elegance of this algorithm is what first drew me in this area of research as a bachelor student!} published in 1977~\cite{KMP} after being independently discovered by Morris-Pratt in a technical report in 1970 and Knuth in 1973. Since then, this has become one of the classic textbook algorithms, and Charras and Lecroq published a detailed handbook~\cite{charras2004handbook} on the various solutions to exact pattern matching.

\subsection{Complex Exact Queries} 

\input{Introduction/fig-matching-models.tex}

However, the need for text processing goes far beyond exact matching of patterns. To illustrate this claim, we present models of matching (underlined) relevant for this thesis with their motivations and specify those we study in the following chapter. Figure~\ref{fig:intro:match_model} also provides an example for each matching model.

% Regular expression
One of the oldest and most classic models for more complex queries is \ul{regular expressions} search, introduced by Kleene in 1951~\cite{RM-704}.
% Explain the formalism
The regular expression formalism offers a concise description for sets of strings through recursive combinations of characters from an alphabet $\Sigma$ along with three fundamental operators: concatenation ($\cdot$), union ($|$), and Kleene star ($\ast$).
%We denote the set of string represented by a regular expression $R$ by $L(R)$. Let $\eps$ denote the empty string.
% characters
%For any character $c \in \Sigma \cup \{\eps\}$, $L(c) = \{c\}$.
% concatenation
%The concatenation of two regular expressions $R_1$ and $R_2$ is defined by $L(R_1 \cdot R_2) = \{s_1s_2$~for~$s_1 \in L(R_1), s_2 \in L(R_2)\}$.
%It's worth noting that the concatenation symbol $\cdot$ is frequently omitted, and the concatenation of $R_1$ and $R_2$ is simply written as $R_1R_2$.
% union
%The union is defined by $L(R_1 | R_2) = L(R_1) \cup L(R_2)$.
% Kleene star
%Finally, the Kleene star applied to a regular expression $R$ is defined as $L(R^\ast)=\{s^n$~for~$s \in L(R), n \in \mathbb{N} \}$.
% automaton
Another way to describe them is through automata with the Thompson automaton construction~\cite{Thompson_automaton}. This automaton can then be simulated efficiently to test whether a string is recognized by the regular expression.
% Application and Limitations
The use of regular expression gained popularity in the 1970s through their efficient implementation in Unix tools such as awk, grep or sed.
They have become a crucial tool in many fields such as internet traffic analysis~\cite{4221791,4579527}, databases, data mining~\cite{1000341,10.5555/645927.672035,10.1145/375551.375569}, computer networks~\cite{10.1145/1159913.1159952}, and protein search~\cite{10.1145/369133.369220}.
Chapter~\ref{chap:regexp} provides a new streaming algorithm for regular expression membership and pattern matching.


% Don't care
Although the versatility of regular expression makes them widely used across fields, they are notoriously difficult to write for users.\todo{lowerbounds details ?}
As a simpler alternative, Fischer and Paterson~\cite{fischer1974string} introduced the \underline{``don't care''} pattern matching where a don't care (also called wildcard or gap) symbol, denoted *, can occur in both the pattern and the text, and matches to any other character of the alphabet (but only one).\todo{change the don't care symbol to '?'}
% Space seed
This model has been directly applied in the PROSITE~\cite{hulo2006prosite} database of proteins where wildcards are supported. More generally space seeds~\cite{li2004patternhunter}, a similar concept where only some positions have to be matched, have been used in homology search~\cite{ma2002patternhunter}, alignment~\cite{david2011shrimp2}, assembly~\cite{birol2015spaced}, and metagenomics~\cite{bvrinda2015spaced}.
% Gapped matching
Patterns with don't cares are sometimes~\cite{lewenstein2011indexing} described as $P= P_1g_1P_2g_2 \dots g_\ell P_{\ell+1}$ where $P_1$,$P_2$,\dots,$P_{\ell+1}$ are patterns over the alphabet $\Sigma$ and $g_1,g_2,\dots,g_{\ell}$ are integers describing the length of the gaps. 
% variable length gap
Naturally this question was later extended to the problem of string matching with \underline{variable length gap}~\cite{bille2012string,bille2014string} where the length of the gaps can vary in intervals $[a_i,b_i]$ for $i\in[1,\ell]$.
% Applications
Note that variable length gaps are also supported to query the PROSITE database.
% variants
Different variants of the problem have been studied~\cite{kopelowitz2016color,cohen2009range,brodal1999finding}, including a simpler version with just two patterns $P_1$ and $P_2$ and a single gap~\cite{peterlongo2006gapped,iliopoulos2009indexing} and the special case $P_1=P_2$~\cite{muthukrishnan2002efficient,keller2007range}.

% consecutive
In 2016, Navarro and Thankatchan~\cite{NAVARRO2016108} proposed an interesting variant, where given a single pattern $P$ and an interval $[a,b]$, one must report all consecutive occurrences of $P$ starting at positions $(i,j)$ (consecutive meaning no other occurrence in between $i$ and $j$) such that $j-i$ belongs to $[a,b]$. Since, consecutive occurrences have been studied in several publications~\cite{DBLP:conf/fsttcs/BilleGPRS20,cpm/BilleGPS21,DBLP:journals/corr/abs-2304-00887,DBLP:journals/corr/abs-2211-16860}.
% Gapped consecutive matching
Recently Bille et al.~\cite{bille2022gapped} proposed a natural combination of the two lines of research: \underline{gapped consecutive matching} where we are given two patterns $P_1$ and $P_2$ as well as an interval $[a,b]$ and must report all consecutive occurrences of $P_1$ and $P_2$ with distance in $[a,b]$.
% Motivations
%This model can be linked to gapped q-grams~\cite{burkhardt2003better} which is an alternative to spaced seeds which were mentioned earlier.
% This thesis
We study gapped consecutive pattern matching in various settings in Chapters~\ref{chap:gapped_stream}, ~\ref{chap:gapped_pm} and~\ref{chap:gapped_index}.


Although an in-depth non-standard matching listing is out of the scope of this manuscript, for completeness, we detail other models found in the literature.
% Degenerate strings
The modelization of flexible and diverse DNA sequences~\cite{comm1970iupac} lead to the model of \underline{degenerate} strings~\cite{abrahamson1987generalized} (also called indeterminate) where each position of the string corresponds to a subset of $\Sigma$.
% (Elastic/Generalised) Degenerate strings
This model has recently been continued in two directions: \underline{elastic degenerate} strings~\cite{iliopoulos2021efficient} where each position is a subset of strings over $\Sigma$ and \underline{generalised degenerate} strings~\cite{alzamel_et_al:LIPIcs:2018:9323} where each position is a subset of strings of $\Sigma^k$ but where the length $k$ can vary from position to position.
%\todo{Detail the biological problem behind the model}
% Weighted string
Alternatively, when probabilities are given for each character and positions in the form of a position weight matrix~\cite{thompson1994clustal} the strings are called \underline{weighted} (or uncertain). Then, the cumulative probability that a string occurs at a starting position is the product of the probabilities of the corresponding characters at each position.
% Abelian/jumbled/many other names 
In the model of \underline{abelian} matching, a string (or a substring) is entirely identified by the letter it contains (with multiplicities), disregarding their order. It stems from the automatic discovery of clusters of genes in genomes where they can occur in a different order but still linked to the same function~\cite{eres2004permutation}, but the same concept has also been used in the context  of using mass spectrometry for DNA assembly~\cite{bocker2003sequencing} where the strings without order are called compomers. This model is also known as jumbled and permutation pattern matching, and several other names, see~\cite{ejaz2010abelian}.
% order preserving
The \underline{order-preserving} model~\cite{kim2014order,kubica2013linear} takes a somewhat opposite approach and says that two strings match if they have the same relative shape: $\forall i,j \in [0,n-1], X[i] < X[j] \leftrightarrow Y[i] < Y[j]$. This matching model aims at capturing the trend detection needed in the stock market and music melody matching problems~\cite{kim2014order}.
%
% Parametrized matching
Another application-driven model is \underline{parametrized strings} or ``p-string'' introduced by Baker~\cite{baker1993theory}, where two strings match if we can transform one into the other by applying a function renaming the parameters, meant to detect code duplication.

\subsection{Repetition Detection}
\todo[inline]{Work in progress do not review!\\ TODO: Detail}
So far we focussed on matching models where we are given a pattern and text as well as conditions that define a match of the pattern in the text. But another central task in text processing is repetitions detection. By repetitions, we refer to consecutive occurrences of the same fragment. They can be repeated twice (a square), three times (a cube) or more, then represented as a run: a maximal periodic substring. They are needed as a theoretical tool to avoid needless repetitive computations, but they also naturally occur in DNA with an important role in genomic fingerprinting~\cite{Kolpakov2003}.
The study of squares in strings goes back to 1906 with the work of Thue~\cite{thue1906} on the construction of an infinite square free word, in Chapter~\ref{chap:squares} we provide an optimal algorithm for square detection.


\subsection{Approximate Queries}
\todo[inline]{Work in progress do not review!\\ TODO: Complete rewrite and add citations about other model with mismatches}
% Similarity measures & approximate matching
Although regular expressions are powerful, Bioinformatics\cite{Gusfield1997}, music analysis~\cite{mongeau1990comparison} and plagiarism detection~\cite{lukashenko2007computer} also need relevant and efficient similarity measures such as the Levenshtein distance~\cite{levenshtein1966binary} or Dynamic Time warping distance~\cite{sakoe1978dynamic}. They also often need to report all occurrences with an error bound\cite{landau1986efficient,landau1989fast}: at a distance at most a threshold $\tau$.
We contribute to this line of research in Chapters~\ref{chap:LCS} and~\ref{chap:DTW}.


\subsection{Scalability Issues}

%%%%%%%%%%%%%%%%%%%%%% Intro scalibility %%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
% But it is not just about the specific model also about scalibility
We discussed so far how string processing tasks have crucial industry-relevant applications, but another major challenge in most applications is the scalability to large datasets.
% Wikipedia
Highly curated datasets generally remain quite small, for example the English pages of Wikipedia (just the text and metadata) take up 20~gigabytes in a compressed format as of 2022~\cite{wikimedia}. In comparison, any form of archival and version history tends to grow much bigger. Just the metadata of revision's history (without the content of the articles) for the Wikipedia English pages takes up 75~gigabytes still as of 2022.
% Software Heritage
It is sometimes possible to limit the redundancy in the archive data, for example by using a graph which tracks where the data is repeated multiple times. This is the approach taken by the Software Heritage~\cite{swh-site} project, which aims at keeping an archive of entirety of the software code produced by humanity. The graph structure is especially necessary in this project to reduce code redundancy and reflect the standard use of version history management in software development.
Substantial research and engineering effort~\cite{DBLP:phd/hal/Pietri21} were made to provide an efficient navigation of the graph. 
However, since the code repositories are indexed based on their URLs and metadata, it is not currently feasible to perform a search specifically for occurrences of a particular code snippet\footnote{\setlength\parindent{10pt} A silly but interesting example is~\cite{vii2014if} where the author searched \texttt{"const double epsilon ="} (and equivalents in other \par languages) on all GitHub repositories to study the value programmers typically chose for epsilon.}.
As of 2023, the graph is limited to 7~terabytes but with the source files the space usage approaches 1~petabyte~\cite{swh-polytechnique}.
% Internet archive
Another example of a large archival project is the internet archive, a non-profit which started saving web pages in 1996 and now holds the history of more than 800 billion web pages through their program: the wayback machine~\cite{web-archive}. This archive takes up more than 70~petabytes, however, one of the limitations is that the search options are limited to the metadata of the websites and not the content of the webpages themselves.


%%%%%%%%%% Bioinformatics
%Intro DNA representation
Large archives also exist in bioinformatics, but the structure of biological sequences is very different from that of code or webpages.
DNA is typically represented as a string over the nucleotide alphabet \texttt{\{A,T,C,G\}} which can be stored using just 2 bits per base. However, this type of storage requires scanning the whole string to search for a pattern. 
At the opposite of the spectrum, a suffix tree enables more efficient sequence analysis but necessitates 10 bytes per base~\cite{navarro2016compact}, which amounting to 30 gigabytes for a human genome containing 3.3 billion bases. 
And thousands of genomes have been sequences through projects such as the 1000 Genomes project~\cite{10002015global} completed in 2015 and the 100K Genomes project~\cite{100Kgenomes} which reached its milestone in 2018. 
A trade-off between those two extremes has been found through the development of compact data structures that exploit redundancy to decrease space usage. For a human genome it allows representing the sequence and its suffix tree using just 4 gigabytes~\cite{navarro2016compact}.
% Redundancy intra genome and intergenome
The redundancy that compact data structures can exploit in genomes can come either from repeated regions in the genome (intra-genome redundancy) or by considering several genomes (from the same specie) that share portions of their genomes (inter-genome redundancy).  
% Reads
However, for DNA, another common data model is the one of reads: when DNA is sequenced, the output is a set of fragments (called \emph{reads}) of the original sequence. Reads can contain sequencing errors, including nucleotide insertions, deletions, and substitutions. The typical length and error rate of the reads vary depending on the sequencing techniques. To be able to reconstruct the original sequence, reads are extracted in such quantities that each position of the original genome is covered multiple times. This means readsets are larger than the assembled genome and even more redundant. In Chapter~\ref{chap:XBWT} we explore this topic for short reads and propose a data structure specifically tailored to the specificities of a sequencing technique. 
% Cheaper = more sequencing
Additionally, the drastic decrease in sequencing cost and increase in throughput since 2008 (faster than expected by Moore's law~\cite{muir2016real}) have lead to higher volumes of DNA being sequenced. 
% ENA
So far, the European Nucleotide Archive has accumulated more than 47 petabytes~\cite{ena} of sequencing data.
% SRA
While the NCBI Sequence Read Archive has more than 73 petabases~\cite{sra} of archive including 38 petabases in open access. However, like for the software heritage project and internet archive, in the ENA and the NCBI, the data is indexed solely based on its metadata.\\
%\todo[inline]{Look for more reasonable size example ? It's unclear if anybody would want to index the entire SRA}

%%%% Astronomical data 
\todo[inline]{Add applications to astronomical data and pulsar detection.}
%\todo[inline]{Add one or several graphs illustrating the growth in data in each field.}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \begin{tikzpicture}
            \node (img) {\includegraphics[width=\textwidth]{0_sheets/wayback_machine.png}};
            \node [below right,text width=2.9cm,align=center, fill=white] at (0.25,-0.5){\footnotesize{Pages removed due to security concerns.}};
            \draw[->] (1,-0.5) -- (1.3,0.3);
        \end{tikzpicture}
        \caption{The Wayback Machine Archive}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{0_sheets/sra_growth.png}
    \caption{The Sequence Read Archive}
    \end{subfigure}
    \caption{Plots of the database growth for the Wayback Machine~\cite{web-archive-growth} and the Sequence Read Archive~\cite{sra}.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%% Approaches to adress scalibility %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
It goes without saying that algorithms with quadratic time complexity cannot scale to terabytes and petabytes of inputs. Consequently, the choice of matching model is dictated both by the specific needs of the application and the typical scale of the input.
%
Several approaches can be used to cope with large amounts of data, including distributed algorithms that are designed to work on several computers sharing a network (with limited transfer). However, this approach is out of the scope of this thesis, instead we focused on sketching. It is a general approach rather than a precise method: a \textbf{sketch} keeps only the essential characteristic of the input needed to answer a given query. 
Sketches aim to be more compact than the original input, offering promising scalability potential. This approach includes lossy and lossless compression. We will detail in Section~\ref{intro:sec:contrib} how each contribution relates to this concept. 
While sketches are a common thread throughout the chapters, we can categorize and summarize their respective scalability approaches as follows (with some approaches that can be combined): 

\begin{itemize}
%\item Distributed algorithms: those are designed to work on several computers sharing a network. The data is processed in parallel with limited transfer over the network (those transfers can be 10 times slower compared to disk transfer). This direction is a research field of its own which
\item Compressed input: as mentioned already, highly redundant data can sometimes be compressed to a manageable size. 
Consequently, algorithms and data structures that can directly operate on the compressed data become much more efficient. This is very natural approach as the data is almost always shared in a compressed format, however not all problems can be solved faster with this approach. For example, Abboud et al.~\cite{abboud2017fine} showed that for computing the longest common subsequence of two strings of uncompressed size $N$, given in a compressed format of size $n$, there is a lower bound $(nN)^{1-o(1)}$ assuming the strong exponential time hypothesis. In other words, even if the input is given compressed form, we cannot avoid a high dependency in the uncompressed size. In this thesis, 
Chapters~\ref{chap:gapped_pm} and~\ref{chap:gapped_index} both take as input a grammar compressed text and their complexities are given as a function of the size of the compressed input.
\item Data structures: here the complexity analysis is split in two parts, the \emph{construction} and the \emph{query}. The construction is generally more expensive and meant to be performed once, whereas the queries are meant to be fast and performed multiple times with varying input-query.
%The size of the data structure refers to the space of the information kept at the end of the construction to perform the query.   
%One of the use case is to construct on a powerful server and then send the small data structure to users that can loaded it in main memory and query it repeatedly. 
Chapters~\ref{chap:gapped_index} and~\ref{chap:XBWT} are examples of that.
\item Efficient second-memory algorithms: One of the challenges when dealing with large inputs is the quantity of main memory used, as most computers are still limited to gigabytes of RAM. To circumvent this limitation, programs may resort to working directly from secondary memory as disk space scales at a much cheaper cost than RAM.
It is  common knowledge that random access to disk is very inefficient, however contiguous reads on recent SSD can read gigabytes (between 2.2 and 3.4 Gb) of data per second which is either comparable to the speed of reading from main memory or only 10 times slower (depending on the model of RAM).
Therefore, an algorithm that uses few random accesses can be executed directly on disk which allows it to scale to large inputs much more easily. The main issue with this approach is that not all problems admit efficient solutions avoiding random access. For example, given a list and a permutation, returning the permuted list requires to access the elements in the order of the permutation, possibly random. We partially use this approach of relying on secondary memory to limit main memory usage in Chapter~\ref{chap:XBWT}. The construction of the index is split in phases that read contiguously from disk, process the information (for the next phase or final output) and write to disk.
%\todo[inline]{If possible I should find a good reference for the speed of RAM vs SSD access, and an example of problem where avoiding random access is difficult.}
\item Streaming algorithms: if the data is so large that it can only be handled as a stream, then it must be processed on the fly without the possibility to go back. Those algorithms are by design efficient on secondary memory, but the character by character is a stronger constraint. Generally the pattern and the length of the text are known in advance and can be preprocessed, then the characters arrive one by one and can only be accessed later if they have been explicitly stored. This model optimizes both the time per character during the streaming phase and the space complexity which accounts for storing the result of the preprocessing and any space necessary to process the characters. Chapter~\ref{chap:regexp} and~\ref{chap:gapped_stream} are set in this model.
\item Approximation algorithms: When dealing with very large datasets, it is not always needed to provide precise answers to queries. Allowing for some approximations in the results can enable shortcuts and help bypass lower bounds.
The entire second part of this thesis is dedicated to this approach, and Chapter~\ref{chap:LCS} stands out as a particularly representative example. We treat the problem of the Longest Common Subsequence with approximately $k$ mismatches with a probabilistic algorithm that answer correctly with high probability. %($\geq 1 - \frac{1}{n}$ for an input of size $n$).
\end{itemize}

\chapter*{Introduction}\label{chap:intro}
\addcontentsline{toc}{chapter}{Introduction}
\chaptermark{Introduction}

When answering the classic question "What is your Ph.D. about?" to family and friends, I always start with the "Ctrl + F" function in their favorite text editor or web browser. This quickly highlights one of the applications of the exact pattern-matching problem. If I feel especially ambitious in my explanations, I will attempt to give the intuition of the naive $\Oh(nm)$ algorithm. Picture a young child, aligning the word against every position of the text and comparing character by character because he has yet to learn how to read. To give a glimpse at a more complex solution, I comment on how, depending on the word, the child may try to skip portions of the text. But even my grandparents immediately know that searching in a text has been possible for decades and that it cannot be my real research subject.

\section{Context}
\subsection{The need for complex queries} 

Indeed, exact pattern matching has been long studied, with in particular the famous Knuth-Morris-Pratt algorithm\footnote{The elegance of this algorithm is what first drew me in this area of research as a bachelor student!} published in 1977~\cite{KMP} after being independently discovered by Morris-Pratt in a technical report in 1970 and Knuth in 1973. Since then, this has become one of the classic textbook algorithms, and Charras and Lecroq published a detailed handbook~\cite{charras2004handbook} on the various solutions to exact pattern matching.

\input{Introduction/fig-matching-models.tex}

However, the need for text processing goes far beyond exact pattern matching. To illustrate my point, I give an overview of various text-processing problems and their motivations. Figure~\ref{fig:intro:match_model} also provides an example for each matching model.
% Regular expression
One of the oldest and most classic models for more complex queries is regular expressions introduced by Kleene in 1951~\cite{RM-704}.
% Briefly explain the formalism
This formalism compactly describes a set of strings recursively starting from three operators, concatenation union and Kleene star.
% Application and Limitations
It has deep connections with automatons~\cite{Thompson_automaton}, and its versatile nature makes it a crucial tool in many fields such as internet traffic analysis~\cite{4221791,4579527}, databases, data mining~\cite{1000341,10.5555/645927.672035,10.1145/375551.375569}, computer networks~\cite{10.1145/1159913.1159952}, and protein search~\cite{10.1145/369133.369220}. Chapter~\ref{chap:regexp} provides a new algorithm for Regular expression and pattern matching.

% Similarity measures
Although regular expressions are powerful, Bioinformatics\cite{Gusfield1997}, music analysis~\cite{mongeau1990comparison} and plagiarism detection~\cite{lukashenko2007computer} also need relevant and efficient similarity measures such as the Levenshtein distance~\cite{levenshtein1966binary} or Dynamic Time warping distance~\cite{sakoe1978dynamic}. They also often need to report all occurrences with an error bound\cite{landau1986efficient,landau1989fast}: at a distance at most a threshold $\tau$.
We contribute to this line of research in Chapter~\ref{chap:LCS} and~\ref{chap:DTW}.
% Don't care
As an alternative, Fischer and Paterson~\cite{fischer1974string} introduced "don't care" matching where a don't care symbol denoted * can occur in both the pattern and the text, matches to any other character of the alphabet (but only one).
% Gapped matching
The "don't care" matching model is sometimes referred to as "gapped" matching; however, it is not to be confused with gapped consecutive matching~\cite{bille2022gapped} where we are given two patterns $P_1$ and $P_2$ as well as an interval $[a,b]$ and must report all occurrences of $P_1$ and $P_2$ with the distance in $[a,b]$ and no occurrences in between. This model also has connections to spaced seeds~\cite{burkhardt2003better}, and we study it in Chapter~\ref{chap:gapped_pm} and~\ref{chap:gapped_index}.

% (Elastic) Degenerate strings
The modelization of flexible and diverse DNA sequences~\cite{comm1970iupac} lead to the model of degenerate string~\cite{abrahamson1987generalized} and more recently elastic degenerate strings~\cite{iliopoulos2021efficient}.
% Abelian/jumbled/many other names 
In the model of Abelian matching, a string (or a substring) is entirely identified by the letter it contains (with multiplicities), disregarding their order. It stems from the automatic discovery of clusters of genes in genomes where they can occur in a different order but still linked to the same function~\cite{eres2004permutation}. This model is also known as jumbled, permutation, compomers matching, and many other names, and Tahir Ejaz dedicated his thesis~\cite{ejaz2010abelian} to this model.
% order preserving
The order-preserving model~\cite{kim2014order,kubica2013linear} takes a somewhat opposite approach and considers that two strings match if they have the same relative shape: $\forall i,j \in [0,n-1], X[i] < X[j] \leftrightarrow Y[i] < Y[j]$. This matching model naturally captures the trend detection in the stock market and music melody matching problems.
%
% Parametrized matching
Another application-driven model is parametrized strings or "p-string" introduced by Baker~\cite{baker1993theory}, where two strings match if we can transform one into the other by applying a one-to-tone function renaming the parameters, meant to detect code duplication.\\

\subsection{Issues of scalability}
% But it is not just about the specific model also about scalibility
We detailed how specific applications can motivate particular string processing tasks, but the scalability in those applications also orient the choice of matching model.

\todo{Example of relevant massive dataset in other area than bioinformatics}

In Bioinformatics, due the cost of next generation sequencing (NGS) has dropped faster than expected by Moore's law~\cite{muir2016real} which led to enormous volume of sequencing data. A quantifiable example of that is the size of the Sequence Read Archive shown in Figure~\ref{fig:intro:sra} since this database tries to capture the major part of the sequencing data produced.

% Why would we need to query the whole archive ?

efficient algorithms and data structures that can leverage the redundant nature of DNA sequences are badly needed. We explore this specific subject in Chapter~\ref{chap:XBWT}

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{Introduction/sra_database_growth.png}
        \caption{Evolution of the number of DNA bases in the Sequence Read Archive (SRA), captured from \href{https://www.ncbi.nlm.nih.gov/sra/docs/sragrowth/}{[the dynamic version on the SRA website]}.}
        \label{fig:intro:sra}
    \end{center}
\end{figure}

\todo[inline]{Try to give other example of scalability issues in other domains}

Several (overlapping) approaches can be used to cope with large amount of data:
\begin{itemize}
\item Distributed algorithms: those are designed to work on several computers sharing a network. The data is processed in parallel with limited transfer over the network (those transfer can be 10 times slower compared to disk transfer). This direction is a research field of its own which is out of the scope of this thesis.
\item Compressed input:  highly redundant data can sometimes be compressed to a manageable size. Then the goal is to design algorithms and data structures that can directly work on the compressed data. Chapter~\ref{chap:gapped_pm} and~\ref{chap:gapped_index} take this approach by working on a grammar compressed text.
\item Data structures: there is first a processing of the input and build of the data structures in a given \emph{construction} time and space. Then the data structures can be queried on various query input. This preprocessing can generally allow for a small \emph{query} time and space of the data structure. One of the use case is to construct on a powerful server and then send the small data structure to users that can loaded it in main memory and query it repeatedly. Chapter~\ref{chap:gapped_index} and ~\ref{chap:XBWT} are examples of that.
\item Efficient second-memory algorithms: random read from disk is very inefficient however contiguous read can be reasonably efficient, being only 100 time slower than reading from main memory~\cite{Gonzalo compact}. Note that even when working in main memory, algorithms that minimize random access to the data are preferable for cache efficiency, but not all problems are well suited to cache efficiency.
\item Streaming algorithms
\item Approximation algorithms
\end{itemize}

\todo[inline]{Detail how we can combine the need for complex queries and scalability.
Periodicity.}

\section{Contributions}\label{intro:sec:contrib}

This thesis makes theoretical and practical contributions to address both the need for efficient complex queries and scalability issues in bioinformatic applications. 
Each contribution is presented as independent chapter corresponding to a publication. This choice was motivated by the variety of subjects and techniques as well as how they were conceived as independent projects (with varying sets of co-authors) during the Ph.D. However, this section is meant to give an overview of the main contributions of the thesis and how they relate.

Extend the abstract description of how the different project relate to each other.

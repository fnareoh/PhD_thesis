\clearemptydoublepage
\bookmarksetup{startatroot}
\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
\chaptermark{Conclusion}
% Contributions
% Context
In this thesis, we presented the need for more general string queries than classical pattern matching, and the scalability challenges that come from the large productions and archrivals of data.
%
In Sections~\ref*{intro:sec:sketching} and~\ref*{intro:sec:contrib}, we detailed the sketch-based approach common to all contributions. The use of sketches enabled processing and storing data efficiently to yield better time and space complexities.
%
The sketching techniques we use are diverse and are not immediate to apply to other problems, but my personal takeaway is the importance of thinking in terms of the key characteristic of the input for a given query. I find it very helpful in algorithmic design both for theoretical studies and applied projects.\\


% Open questions in many of the works
Let me recall briefly our contributions and the open questions they leave. 
In Chapter~\ref{chap:regexp}, we gave streaming algorithms for solving the regular expression pattern matching and membership problems. For a regular expression $R$ of size $m$ in a stream of size $n$, our algorithms run in $\Oh(d^3 \polylog n)$ space and $\Oh(nd^5\polylog n)$ time per character where $d$ is the number of $|$ and $\ast$ symbols in $R$. Is it possible to achieve $\mathrm{poly}(d,\log n)$ space and time per character?

Chapters~\ref{chap:gapped_index} and~\ref{chap:gapped_pm} studied gapped consecutive occurrences, where given a text $T$,  patterns $P_1$ and $P_2$ and an interval $[a,b]$, the task is to find in $T$ all consecutive occurrence of $P_1$ and $P_2$ separated by a distance $d \in [a,b]$. In both chapters, the text $T$ is given as a straight-line program (a grammar generating a single string) of size $g$, but Chapter~\ref{chap:gapped_index} focuses on indexing, while Chapter~\ref{chap:gapped_pm} studies pattern matching. In Chapter~\ref{chap:gapped_index}, we gave two indexes: an index for reporting consecutive occurrences without constraints on the distance, using $\Oh(g^2\log^4|T|)$ space, and an index for reporting consecutive occurrences at distance within $[0,b]$ taking $\Oh (g^5\log^5(|T|))$ space. The query time is $\Ohtilde(|P_1|+|P_2|+\occ)$ for both indexes. 
%
Can we improve the space complexity of our indexes, in particular, is space $\Ohtilde(g)$ achievable with the same time complexity for consecutive (without distance constraints)? Is there an efficient index for the general case where we search for consecutive occurrences separated by a distance in an interval $[a,b]$?

In Chapter~\ref{chap:gapped_pm} we addressed pattern matching where the pattern and the text are processed simultaneously. We showed how to report all consecutive occurrences in $\Oh(g+|P_1|+|P_2|+\occ)$ optimal time and how we can filter the consecutive occurrences to report those at distance in $[a,b]$ in the same complexity.

The last contribution of Part~\ref{part:complex_queries} (Chapter~\ref{chap:squares}), was a deterministic algorithm for reporting runs in $\Oh(n\log\sigma)$ time for unordered alphabet (where the only operation allowed on characters is equality testing) and we showed a matching lower bound for deterministic algorithms. It remains open whether the lower bound of $\Omega(n\log \sigma)$ comparisons for square testing holds for randomized algorithms.

Chapter~\ref{chap:LCS} focused on the \kApproxLCS problem, where for a constant $\eps > 0$ and given two strings $X$ and $Y$ and an integer $k$, we must return a substring of $X$ of length at least $\lcsk(X,Y)$ that occurs in $Y$ with at most $(1+\eps) \cdot k$ mismatches. We provided two algorithms: one assuming a constant size alphabet running in $\Oh(n^{1+ 1/(1+2\eps) + o(1)})$ time and space, and one in $\Oh(n^{1+1/(1+\eps)} \log^3 n)$ time and linear space without constraints on the alphabet.  We also confirmed the practicality of the second algorithm by an experimental evaluation.
As future work, it would be interesting to implement our $\Oh(n^{1+ 1/(1+2\eps) + o(1)})$ time and space solution using an implementation of Approximate Nearest Neighbour data structure such as~\cite{spotify_annoy} and add it to the practical evaluation.

Next, we studied pattern matching for the Dynamic Time Warping (DTW) distance. In Chapter~\ref{chap:DTW}, for a run-length compressed pattern $P$ with $m$ runs and a run-length compressed text $T$ with $n$ runs and an integer $k$, we provided an $\Oh(knm)$-time algorithm that computes all locations $j$ where the DTW distance between $P$ and a suffix of $T[..j]$ is at most $k$. It remains open whether it is possible to improve to $\Oh(k(n+m))$ time. Additionally, we detailed a possible practical application of DTW for third-generation sequencing alignment through minimal experiments. It
would be interesting to investigate further, unfortunately, it seems difficult due to many tools using an alignment based on the edit distance under the hood.    

Finally, in Chapter~\ref{chap:XBWT}, we studied the applicative problem of read indexing. We proposed using the read's alignment to give additional context and to achieve better overall compression. Our index is based on the XBWT, and we provided a main memory-efficient construction using prefix-free parsing. We measured the improvement of our structure in terms of the number of runs in the XBWT as a first step, but it remains to implement and evaluate the full data structure in terms of space usage and query time.\\


From a more general point of view, there remains a lot to be done with sketches. In this thesis, we heavily used the Karp--Rabin fingerprints to improve the efficiency of our algorithms and this is likely extendable to many other problems. Another approach common to Chapters~\ref*{chap:gapped_index},~\ref*{chap:gapped_pm} and~\ref*{chap:DTW} was working on compressed input (either a straight-line program or a run-length compressed string) and there again I believe there is more to be done. I find this direction especially interesting because of the practical perspective of being able to always work directly on the compressed data without the need to decompress. Several of the classical processing tasks have been implemented already for a straight-line program input, but, in practice, grammar compression requires large construction time and space, an order of magnitude more than to construct the Lempel--Ziv factorization (See column ``Re-pair'' of Fig.9 and 10 of~\cite{DBLP:journals/jcss/ClaudeNP21}). So could we improve construction time for grammar compression or develop some algorithms working directly on the LZ factorization?


On the more practical side, I would personally be interested in the use of spaced seeds~\cite{li2004patternhunter} in bioinformatics. A space seed is a binary sequence that describes positions that are either relevant (marked by a one) or irrelevant (marked by a zero), then two strings match for the spaced seed if they match at every relevant position.
They have been used for homology search~\cite{ma2002patternhunter}, alignment~\cite{david2011shrimp2}, assembly~\cite{birol2015spaced}, and metagenomics~\cite{bvrinda2015spaced}. In all those applications they were reported to increase the sensitivity and specificity performances compared to the use of $k$-mers, but they cannot be hashed as efficiently as $k$-mers and their use incurred a greater computational cost. Recently, Petrucci et al.~\cite{petrucci2020iterative} proposed a technique that greatly speeds up space seed hashing. Consequently, I wonder if sketching techniques used in bioinformatics (presented at the end of Section~\ref{intro:sec:sketching}) could gain accuracy by being based on spaced seeds rather than $k$-mers.

\backmatter